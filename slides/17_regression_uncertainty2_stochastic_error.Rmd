---
title: "Regression and uncertainty part 2: stochastic error"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(MASS)
library(tidyverse)
library(broom)
select<-dplyr::select
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
# set global options
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "tiny")
```

## The linear regression model

\[ Y = \beta_0 + \beta_1 X + \varepsilon \] \pause

$\beta_0$: The value of $y$ when $x$ is equal to zero \pause

$\beta_1$: The average increase in $y$ when $x$ increases by one unit \pause

$\varepsilon$: The distance between the line $y = \beta_0 + \beta_1 X$ and the actual observed values of $y$. Allows us to estimate the line, even when x and y do not fall exactly on a line. \pause

## Understanding the regression line

```{r echo = FALSE, size = "tiny", fig.height = 2}
r<-0.95
sample <- mvrnorm(n=10, 
                  mu=c(1, 1), 
                  Sigma=matrix(c(1, r, r, 1), nrow=2), 
                  empirical=TRUE)

sample<-data.frame(x = sample[,1], y = sample[,2])

m1<-lm(y~x, data = sample)
coefs<-coef(m1)

as_tibble(sample)

sample$yhat<-coefs[1] + coefs[2] * sample$x
sample$residual<-sample$y - sample$yhat
```

$\hat{\beta_0} =$ `r coefs[1]`, $\hat{\beta_1} =$ `r coefs[2]`

- Estimate $\hat{Y}$. Recall that $\hat{Y} = \hat{\beta_0} + \hat{\beta_1} X$
- Estimate $\varepsilon$. Recall that $\varepsilon = Y - \hat{Y}$

## Understanding the regression line

$\hat{\beta_0} =$ `r coefs[1]`, $\hat{\beta_1} =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() #+ 
  #geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  #geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding the fit

$\hat{\beta_0} =$ `r coefs[1]`, $\hat{\beta_1} =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) #+ 
  #geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding $\hat{y}$

$\hat{\beta_0} =$ `r coefs[1]`, $\hat{\beta_1} =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding $\varepsilon$

$\hat{\beta_0} =$ `r coefs[1]`, $\hat{\beta_1} =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2, size = 2)  +
  geom_segment(aes(x = x, xend =x, y = y, yend = yhat), lty =2)
```

## Assumptions of a linear regression model

For estimates of $\beta$ to be unbiased and consistent, the following assumptions must be met:

1. The linear model approximates the data generating process \pause
2. Exogeneity of errors: $E(\varepsilon)=0$, errors uncorrelated with predictors \pause
3. Linear independence of predictors \pause
4. Constant error variance (Homoskedasticity): $V(\varepsilon|X)=V(\varepsilon)$

## Assumptions of a linear regression model

For estimates of $\beta$ to be unbiased and consistent, the following assumptions must be met:

1. The linear model approximates the data generating process \pause
2. Exogeneity of errors: $E(\varepsilon)=0$, errors uncorrelated with predictors \pause
3. Linear independence of predictors \pause
4. **Constant error variance (Homoskedasticity):** $V(\varepsilon|X)=V(\varepsilon)$

## Ways to express an OLS model 

As linear with Normal errors:

\[y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots \varepsilon \]
\[\varepsilon \sim N(0,\sigma^2)\]
\pause
As Normal, with linear vector of means:

\[y \sim N(\beta X, \sigma^2)\]

## What this means: 95% of observations should fall in this zone

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2, size = 2)  +
  geom_segment(aes(x = x, xend =x, y = y, yend = yhat), lty =2) + 
  geom_ribbon(aes(x = x,
                  ymin = yhat - 2 * summary(m1)$sigma,
                  ymax = yhat + 2 * summary(m1)$sigma),
              alpha = 0.25)
```

## One way to visualize: residuals vs fitted

```{r, echo = F}
df1<-data.frame(fitted = fitted(m1),
                residuals = resid(m1))

ggplot(df1, aes(x = fitted, y = residuals)) + 
  geom_point() + 
  geom_hline(yintercept = 0, lty = 2)

```

# Let's try this with real data

```{r echo = F}
dat<-read_csv("https://www.openintro.org/data/csv/yrbss_samp.csv")
### check values on focal measures
unique(dat$hours_tv_per_school_day)
unique(dat$school_night_hours_sleep)

#### do some cleaning
dat<-dat %>% 
  mutate(hours_tv_per_school_day = 
           as.numeric(
             case_when(
               hours_tv_per_school_day == "<1" ~ "0.5",
               hours_tv_per_school_day == "5+" ~ "5",
               hours_tv_per_school_day == "do not watch" ~ "0",
               T ~ hours_tv_per_school_day
             )))

dat<-dat %>% 
  mutate(school_night_hours_sleep = 
           as.numeric(
             case_when(
               school_night_hours_sleep == "<5" ~ "4",
               school_night_hours_sleep == "10+" ~ "10",
               T ~ school_night_hours_sleep
             ))) %>% 
  select(hours_tv_per_school_day, school_night_hours_sleep) %>% 
  na.omit(dat)

dat
```

## Fit a model for sleep duration predicted by tv watching

```{r}
m1<-lm(school_night_hours_sleep ~ hours_tv_per_school_day,
       data = dat)

tidy(m1)
```

## Evaluate the regression line

```{r echo = F}
ggplot(dat,
       aes(x = hours_tv_per_school_day,
           y = school_night_hours_sleep)) + 
  geom_jitter(width = 0.05, height = 0.05) + 
  geom_point(aes(y = fitted(m1)), color = "red") + 
  geom_smooth(method = "lm", se = F)
```

## What is the distribution of the residuals? Are they Normal?

```{r fig.height = 4}
ggplot(data = data.frame(residuals = resid(m1)),
       aes(x = residuals)) + 
  geom_density()
```

## What about residuals vs fitted? 

```{r fig.height = 4}
ggplot(data = data.frame(fitted = fitted(m1),
                         residuals = resid(m1)),
       aes(x = fitted, y = residuals)) + 
  geom_point()+ 
  geom_hline(yintercept = 0, lty = 2)
```

## Looks ok! Now what? 

Because our model is not *heteroskedastic* (non-constant error variance), we can make valid predictions from it! \pause

Before, we estimated the sampling distribution of $E(y)$ using information on the sampling distributions of $\hat\beta_0$ and $\hat\beta_1$ \pause

But that's not the only source of uncertainty in our model!

## Sources of random variation in our model

$\hat \beta_0 \sim N(\beta_0, s^2_{\beta_0})$
$\hat \beta_1 \sim N(\beta_1, s^2_{\beta_1})$
$y = \beta_1 + \beta_2 + \varepsilon$
$\varepsilon \sim N(0, \sigma^2)$

# What does epsilon represent? 

# Exercise: Make predictions from m1 for y. 

## What does non-constant error variance look like?

Data with non-constant error variance will show distinctive patterns in their residuals

```{r}
library(gapminder)
m2<-lm(lifeExp ~ gdpPercap, 
       data = gapminder)
```

## Non-contant error variance

```{r fig.height=4}
ggplot(data.frame(fitted = fitted(m2),
                  residuals = resid(m2)),
       aes(x = fitted, y = residuals)) + 
  geom_point() + 
  geom_hline(yintercept = 0, lty = 2)
```

## Problems

Here there are actually two problems:

1. The linear model does not reflect the data generating process (the relationship between x and y)
2. Error variance is not constant

## OK, so log it

```{r}
library(gapminder)
m3<-lm(lifeExp ~ log(gdpPercap), 
       data = gapminder)
```

## Non-contant error variance

```{r fig.height=4}
ggplot(data.frame(fitted = fitted(m3),
                  residuals = resid(m3)),
       aes(x = fitted, y = residuals)) + 
  geom_point() + 
  geom_hline(yintercept = 0, lty = 2)
```

## In summary

In linear regression, we assume that the residuals follow a normal distribution. \pause

We can write this two ways (they are equivalent):

1. $y = \beta_0 + \beta_1 X + \varepsilon; \varepsilon \sim N(0, \sigma^2)$
2. $\mu = \beta_0 + \beta_1 X; y \sim N(\mu, \sigma^2)$

## In summary

We have multiple sources of random error in our model. 

1. We have uncertainty in our estimates of $\beta$, that we approximate using the Central Limit Theorem. This is the uncertainty we have about the location of $E(y)$ or $\mu$
2. We have uncertainty in $y$, which is ordinary sampling error. For linear regression, we are going to assume that conditional on $X$, this error is Normally distributed