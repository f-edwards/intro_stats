---
title: "Uncertainty, 1"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(pander)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "tiny")
```

## The basic challenge: Will Obama win Florida in '08?

```{r echo = F, fig.height = 4}
data(polls08)
temp<-polls08 %>% 
  filter(state=="FL") %>% 
  select(Obama, McCain) %>% 
  gather() %>% 
  rename(Candidate = key, support = value)

head(polls08 %>% filter(state=="FL"))

ggplot(temp, 
       aes(x = support, fill = Candidate)) + 
  geom_density(alpha = 0.5)
```

## To estimate a proportion of voters supporting a candidate

- Take a simple random sample of $n$ voters \pause
- Ask them their preferences (support for Obama $x=[0,1]$) \pause
- Compute the proportion of voters supporting Obama in the sample $\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$ \pause    
- Use $\bar{x}$ to inform our understanding of $p$, the true proportion of voters supporting Obama

## Is our estimate biased?

\[\textsf{estimation error} = \textsf{estimate} - \textsf{truth} = \bar{x}-p\] \pause

Problems?

## Obtain average error, when exact error is unobservable

The data: 

\[x \sim \textsf{Bernoulli}(p)\]

## A single sample 

```{r}
sample<-rbinom(size=1, n=10, prob=0.55)
### x_i
sample
### \bar{x}
sum(sample)/length(sample)
```

What is the estimation error?

## Treat the sample mean $\bar{x}$ as a random variable

Recall that a Binomial variable represents the number of successes from $n$ Bernoulli trials with a probability of success $p$ \pause

We can represent the average share of Obama supporters in a sample of voters as a Binomial random variable:

\[\bar{x} \times n \sim \textsf{Binomial}(p,n)\] \pause

What does this probability distribution mean?

## Repeat our sample of voters

```{r}
sample1<-rbinom(size=1, n=10, prob=0.55)
sample2<-rbinom(size=1, n=10, prob=0.55)
sample3<-rbinom(size=1, n=10, prob=0.55)
mean(sample1); mean(sample2); mean(sample3)
```

## We can repeat this sampling a large number of times

Recall the Law of Large Numbers: as a sample of draws from a random variable increases, the sample mean converges to the population mean $E(X)$

\[\bar{x}_n \rightarrow E(X) \]


## n = 5


```{r fig.height=3}
sim_dat<-data.frame(x = rep(NA, 5))
for(i in 1:5){
  sample<-rbinom(size=1,n=10,prob=0.55)
  sim_dat$x[i]<-mean(sample)
}
ggplot(sim_dat, aes(x=x)) +
  geom_histogram(bins=30) + 
  geom_vline(xintercept = 0.55, lty=2)
```

## n = 10


```{r fig.height=3}
sim_dat<-data.frame(x = rep(NA, 10))
for(i in 1:10){
  sample<-rbinom(size=1,n=10,prob=0.55)
  sim_dat$x[i]<-mean(sample)
}
ggplot(sim_dat, aes(x=x)) +
  geom_histogram(bins=30) + 
  geom_vline(xintercept = 0.55, lty=2)
```

## n = 100


```{r fig.height=3}
sim_dat<-data.frame(x = rep(NA, 100))
for(i in 1:100){
  sample<-rbinom(size=1,n=10,prob=0.55)
  sim_dat$x[i]<-mean(sample)
}
ggplot(sim_dat, aes(x=x)) +
  geom_histogram(bins=30) + 
  geom_vline(xintercept = 0.55, lty=2)
```

## n = 1000

```{r fig.height=3}
sim_dat<-data.frame(x = rep(NA, 1000))
for(i in 1:1000){
  sample<-rbinom(size=1,n=10,prob=0.55)
  sim_dat$x[i]<-mean(sample)
}
ggplot(sim_dat, aes(x=x)) +
  geom_histogram(bins=30) + 
  geom_vline(xintercept = 0.55, lty=2)
```

## Compare this to Binomial(n,p)

```{r fig.height=3}
x<-data.frame(x=0:10, 
              y=dbinom(0:10,size=10,prob=0.55))
ggplot(x, aes(x=x,
              y=y)) + 
  geom_col() + 
  geom_vline(xintercept = 0.55 * 10, lty=2)
```

## Treating an estimator as a random variable

\[\bar{x}_n \rightarrow E(X) \]

- So if we resample $x$ from the same population a large number of times, then compute $\bar{x}$, the distribution of sampled values of $\bar{x}$ approximates the *sampling distribution* of $\bar{x}$ \pause
- This holds under simple random sampling, where each unit has an equal probability of being selected

## So is our sample biased?

\[\textsf{estimation error} = \textsf{estimate} - \textsf{truth} = \bar{x}-p\] 
\pause
\[\textsf{bias} = E(\textsf{estimation error}) = E(\textsf{estimate} - \textsf{truth}) = E(\bar{x}) - p\]

\pause

Because $E(\bar{x}) = p$ under simple random sampling

\[\textsf{bias} = E(\bar{x}) - p = p-p =0\]  \pause

The sample mean $\bar{x}$ is, on average, an *unbiased* estimator of the population average $p$ as long as we've obtained a simple random sample. \pause

**However:** any individual sample has unobservable error $\bar{x_n}-p$

## Bias and consistency

An estimator is biased when $E(\bar{x}) \neq p$

- Bias means that the expectation is systematically different from the true value
- Positive bias means that $E(\bar{x}) > p$, negative bias means $E(\bar{x}) < p$

An estimator is consistent if it converges to the parameter as n increases, or \[\bar{x}_n \rightarrow E(X)\]

## In practice

- Simple random sampling yields unbiased and consistent estimators of a parameter \pause
- In experimental settings: Random assignment yields an unbiased estimator of the Sample Average Treatment Effect \pause
- In experimental settings: Simple random sampling combined with random assignment yields an unbiased and consistent estimator of the Population Average Treatment Effect \pause
- See pages 317-322 for more discussion of RCTs and bias in estimators

## How wrong are we, on average?

For our vote share example, pretend that $p = 0.55$, and $n=10$. For a Binomial random variable expressed as a proportion, the variance is:

\[\sigma^2 = \frac{p(1-p)}{n} = \frac{0.55(1-0.55)}{10} \approx 0.025\] \pause

We define a standard error as the standard deviation of a sampling distribution

\[SE = \sigma = \sqrt{0.025} \approx 0.16\] \pause

When we sample 10 voters, our estimate of the share of the population that supports Obama will have an average error of about 0.16, or 16 percentage points. 

What happens as we increase n?

## The impact of n on a standard error for x~Binomial(n, 0.55)

```{r echo = F}
t<-data.frame(n = 2:100)
t$se<-sqrt((0.55 * (1-0.55))/t$n)
ggplot(t, aes(x = n, y = se)) + 
  geom_line() + 
  labs(x = "n", y = "Standard Error")
```


## The Standard Error

In general, the standard error of a sample mean $\bar{x_n}$ where $x$ is a sample of $n$ i.i.d random variables is:

\[SE(\bar{x}) = \frac{sd_x}{\sqrt{n}}\] \pause

We use the observed data to approximate the standard deviation of the sampling distribution for a population parameter, such as a mean. \pause

It tells us how far away $\bar{x}$ is from $E(x)$ on average

## Describing a distribution 

Let's return to our voter polls. Assume each of these polls is a simple random sample of voters.

```{r}
data(polls08)
fl<-polls08 %>% 
  filter(state=="FL") %>% 
  select(Obama) 

head(fl)
```

\pause 

Each of these represents a draw from the sampling distribution of voters in Florida. The vote share estimate is the mean of a group of sampled voters, whose preference for Obama is a Bernoulli variable with probability $p$

## The Central Limit Theorem

The sum of i.i.d. Bernoulli trials is Binomial(n,p). When we add i.i.d. Binomial random variables together, the Central Limit Theorem shows that they converge toward a Normal distribution as the number of variables increases. \pause

```{r fig.height = 4}
ggplot(fl, aes(x=Obama)) + 
  geom_histogram()
```

## The Central Limit Theorem

More formally:

\[\bar{x}_n \sim N\left(E(x), \frac{V(x)}{n}\right) \textsf{ as } n \rightarrow \infty \]

The sampling distribution of $\bar{x}$ is approximately Normal, if we resample $x$ many times. \pause

Using our polling data, we obtained a distribution of $\bar{x}$ with a mean of 46.1 and standard error of 3.4

## Observed sample means (blue), and N(46.1, 3.4)

```{r echo = FALSE}
ggplot(data.frame(x=c(30,60)),
       aes(x=x)) + 
  stat_function(fun=function(.x) dnorm(.x, 46.1, 3.4)) + 
  geom_density(data = fl, aes(x=Obama), fill = "blue", alpha = 0.5)
```

## How certain are we about our estimate?

To provide an estimate of how certain we are about our estimate, we can compute a *confidence interval*.

1. Choose a a confidence level (e.g. 0.9, 0.95, 0.99) \pause
2. Compute the quantile of the Normal distribution that corresponds to that confidence level (symmetric) \pause
3. Multiply the value of this quantile by the standard error of the quantity, then add and subtract this value from the sample mean

## Confidence interval example

Obtain a 90 percent confidence interval of Obama vote share in Florida (2008)

\[\alpha = 1-0.9 = 0.1\] \pause

Because a Normal is symmetric, we want to obtain all of the probability mass of the Normal curve above the 0.05 quantile and below the 0.95 quantile, or $0+\alpha/2$ and $1-\alpha/2$. \pause

On the $z$ score $N(0,1)$ scale, this corresponds to 

```{r}
qnorm(0.05, 0, 1)
qnorm(0.95, 0, 1)
```

## Confidence interval example

Our 90\% confidence interval is equal to:

\[\bar{x} \pm z_{\alpha/2} \times SE\]

```{r}
mean(fl$Obama) - 1.64*sd(fl$Obama)
## and
mean(fl$Obama) + 1.64*sd(fl$Obama)
```

## Increase the confidence level to 0.95

Now we need the z-score for a Normal CDF at 0.975 and 0.025, and obtain our interval as before

```{r}
## 90 percent interval
probs<-qnorm(c(0.05, 0.95), 0, 1)
mean(fl$Obama) + probs*sd(fl$Obama)
## 95 percent interval
probs<-qnorm(c(0.025, 0.975), 0, 1)
mean(fl$Obama) + probs*sd(fl$Obama)

## 99 percent interval
probs<-qnorm(c(0.005, 0.995), 0, 1)
mean(fl$Obama) + probs*sd(fl$Obama)

```

What do you notice?

## Confidence intervals for $\alpha = {0.1, 0.05, 0.01}$

```{r echo = FALSE}
ggplot(data.frame(x=c(46.1 -15,46.1+15)),
       aes(x=x)) + 
  stat_function(fun=function(.x) dnorm(.x, 46.1, 3.4)) + 
   stat_function(fun=function(.x) dnorm(.x, 46.1, 3.4),
                xlim=mean(fl$Obama) + 
                  qnorm(c(0.05, 0.95), 0, 1) *
                  sd(fl$Obama),
                geom="area",
                fill="blue",
                alpha = 0.75) +
  stat_function(fun=function(.x) dnorm(.x, 46.1, 3.4),
                xlim=mean(fl$Obama) + 
                  qnorm(c(0.025, 0.975), 0, 1) *
                  sd(fl$Obama),
                geom="area",
                fill="blue",
                alpha = 0.5) +
    stat_function(fun=function(.x) dnorm(.x, 46.1, 3.4),
                xlim=mean(fl$Obama) + 
                  qnorm(c(0.005, 0.995), 0, 1) *
                  sd(fl$Obama),
                geom="area",
                fill="blue",
                alpha = 0.25)
```

## Interpreting confidence intervals

What does this interval mean?

```{r}
## 95 percent interval
probs<-qnorm(c(0.025, 0.975), 0, 1)
mean(fl$Obama) + probs*sd(fl$Obama)
```

\pause

It means that if we draw another sample from this same population, the sampled value will fall within the computed interval 95 percent of the time. \pause

It does *not* mean that probability that the true value of the population mean falls within the interval is 0.95. \pause

It does mean if we draw a large number of samples and construct 95 percent intervals, they will contain the population mean 95 percent of the time.

## Confidence intervals for effect sizes in experiments

Let's return to the class-size experiment. We can use a confidence interval to show how uncertain our effect estimates are.

```{r ehco = FALSE, fig.height =3}
data(STAR)
STAR<-STAR %>% 
  mutate(classtype = ifelse(classtype==1, "small", 
                            ifelse(classtype==2, "regular",
                                   NA))) %>% 
  filter(!(is.na(classtype)), !(is.na(g4reading)))
ggplot(STAR, aes(x=g4reading, fill = classtype)) + 
  geom_density(alpha =0.5)
```

## Confidence intervals for each group

```{r}
STAR_groups<-STAR %>% 
  group_by(classtype) %>% 
  summarise(xbar_g4reading = mean(g4reading),
            students = n(),
            sd = sd(g4reading),
            se_g4reading = sd(g4reading)/sqrt(n()))
```

And 95\% Confidence Intervals for each group

```{r}
alpha = 0.05
STAR_groups %>% 
  mutate(ci_lower = xbar_g4reading - qnorm(1-alpha/2, 0, 1)*se_g4reading,
         ci_upper = xbar_g4reading + qnorm(1-alpha/2, 0, 1)*se_g4reading)
```

## Confidence interval for the SATE

```{r}
## sample average treatment effect
SATE<-STAR_groups$xbar_g4reading[2]-STAR_groups$xbar_g4reading[1]
SATE
```

\pause

```{r}
## SE of the SATE
SATE_se<-sqrt(STAR_groups$se_g4reading[1]^2 + STAR_groups$se_g4reading[2]^2)
SATE_se
```

\pause

```{r}
### 95 percent CI
SATE+qnorm(1-alpha/2, 0, 1) *SATE_se
SATE-qnorm(1-alpha/2, 0, 1) *SATE_se
```

This interval is pretty wide. It suggests we don't have much certainty about whether the treatment has an effect.
$[-1.7, 8.7]$.  

## The estimated sampling distribution of the SATE, with 95 percent interval

```{r echo =F}
ggplot(data.frame(x=c(SATE-8, SATE+8)), aes(x=x)) +
  stat_function(fun=function(.x) dnorm(.x, SATE, SATE_se)) +
  stat_function(fun=function(.x) dnorm(.x, SATE, SATE_se),
                xlim=c(SATE - 
                  qnorm(1-alpha/2, 0, 1) *
                  SATE_se,
                  SATE + 
                  qnorm(1-alpha/2, 0, 1) *
                  SATE_se),
                geom="area",
                fill="blue",
                alpha = 0.5)
```

## Hypothesis testing

We would like to know whether assignment to a small class in Kindergarten improves reading scores in fourth grade.  \pause

We start by proposing two hypotheses:

- $H_0$: Kindergarten class assignment has no effect on fourth grade reading scores
- $H_1$: Kindergarten class assignment has an effect on fourth grade reading scores

We will use the sampling distribution of the SATE to test this hypothesis

## General approach

1. Specify a null and alternative hypothesis \pause
2. Choose a test statistic and level of $\alpha$ (conventionally, 0.05) \pause
3. Derive the sampling distribution of the test statistic under the null hypothesis \pause
4. Compute a p-value (the probability that, under the null hypothesis, we would observe a result as extreme as we observed in the data) \pause
5. Reject the null hypothesis if $p\leq\alpha$, otherwise retain the null hypothesis

## For the STAR experiment

\[H_0: \mu_{small} - \mu_{regular} = 0 \] \pause
\[H_1: \mu_{small} - \mu_{regular} \neq 0 \] \pause

- Under the null hypothesis, we assume that the SATE has a Normal distribution with mean 0 and standard deviation equal to the observed standard error of the SATE \pause
- Let's go with the conventional $\alpha = 0.05$ threshold (there's nothing special about it though!) \pause
- Then we estimate the probability that we could observe the data under the null hypothesis using the Normal Cumulative Distribution Function (pnorm in R)

## Rejection region: The sampling distribution of the SATE under the null hypothesis

We will reject the null hypothesis if our test statistic falls in either of the red areas (two-tailed test):

```{r fig.height = 4, echo=F}
alpha<-0.05
ggplot(data.frame(x=c(-5,5)), aes(x=x)) +
  stat_function(fun=function(.x) dnorm(.x, 0, 1),
                geom = "area",
                fill = "red",
                color = "black") +
  stat_function(fun=function(.x) dnorm(.x, 0, 1),
                xlim=c(0 - 
                  qnorm(1-alpha/2, 0, 1) *
                  1,
                  0 + 
                  qnorm(1-alpha/2, 0, 1) *
                  1),
                geom="area",
                fill="white",
                color = "black")
```

## Let's calculate our p-value

Because the Normal CDF is symmetric, we use the left-tail to estimate the probability that it falls within the rejection zone on either side

```{r}
2* pnorm(-abs(SATE), 0, SATE_se)
```

Can we reject $H_0$?

## Making it easier on ourselves

R has built in functions for significance tests (t-tests)

```{r}
STAR_small<-STAR %>% 
  filter(classtype=="small") 
STAR_regular<-STAR %>% 
  filter(classtype=="regular")
t.test(STAR_small$g4reading, 
       STAR_regular$g4reading)
```

## Words of warning

- Hypothesis tests and confidence intervals are tricky to interpret, and are often intrepreted incorrectly \pause
- Hypothesis testing has become a *very* problematic feature of modern social science \pause
- Just because a result is has a p-value of less than $0.05$ does not mean that a finding is true! \pause
- The routines of modern science exacerbate these problems \pause

If we test 10 hypotheses in an analysis, the likelihood that at least one will be significant is:

```{r}
1 - 0.95^10
```

## Homework and lab

- For homework, you will compute confidence intervals and conduct significance tests using the polls08 data. The assignment will be posted in HW9 on slack
- Lab will review how to compute standard errors, confidence intervals, and significance tests

# Lab

## Returning to the Pager data

```{r}
dat<-read_csv("./data/criminalrecord.csv")
head(dat)
```

## Compute the sample average callback rate

- For Black and white testers

```{r}
bw<-dat %>% 
  group_by(black) %>% 
  summarise(callback = mean(callback))
```


- For those assigned to criminal record treatments

```{r}
cr<-dat %>% 
  group_by(crimrec) %>% 
  summarise(xbar = mean(callback))
```

- For criminal record treatments for both Black and white testers

```{r}
bw_cr<-dat %>% 
  group_by(black, crimrec) %>% 
  summarise(xbar = mean(callback))
```

## Now compute the standard error for each group

- For Black and white testers

```{r}
bw<-dat %>% 
  group_by(black) %>% 
  summarise(xbar = mean(callback),
            se = sd(callback)/sqrt(n()))
```


- For those assigned to criminal record treatments

```{r}
cr<-dat %>% 
  group_by(crimrec) %>% 
  summarise(xbar = mean(callback),
            se = sd(callback)/sqrt(n()))
```

- For criminal record treatments for both Black and white testers

```{r}
bw_cr<-dat %>% 
  group_by(black, crimrec) %>% 
  summarise(xbar = mean(callback),
            se = sd(callback)/sqrt(n()))
```

## Now compute critical values for confidence intervals for each group

- At the 90\% level

```{r}
## critical values are (1 - alpha)/2
## 95 percent (1 - 0.95)/2
q_90<-qnorm(0.95, mean = 0, sd = 1)
q_95<-qnorm(0.975, mean = 0, sd = 1)
q_99<-qnorm(0.995, 0, 1)

bw<-dat %>% 
  group_by(black) %>% 
  summarise(xbar = mean(callback),
            se = sd(callback)/sqrt(n())) 

bw %>% 
  mutate(ci_90_lwr = xbar - se * q_90,
         ci_90_upr = xbar + se * q_90,
         ci_95_lwr = xbar - se * q_95,
         ci_95_upr = xbar + se * q_95,
         ci_99_lwr = xbar - se * q_99,
         ci_99_upr = xbar + se * q_99)
  
```


- At the 95\% level
- At the 99\% level

## Now compute a significance test at the 0.05 level for this hypothesis

Where $w$ indicates white testers and $b$ indicates Black testers

- $H_0: \mu_{b} = \mu_w$

note, this is equivalent to $H_0: \mu_b - \mu_w = 0$

```{r}
dat_w<-dat %>% 
  filter(black==0)
dat_b <- dat %>% 
  filter(black==1)

t_test<-t.test(dat_w$callback, dat_b$callback)


sate <- bw$xbar[1] - bw$xbar[2]
se <- sqrt(bw$se[1]^2 - bw$se[2]^2)

2 * pnorm(-abs(sate), 0, se)

```

