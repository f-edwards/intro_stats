---
title: "Probability, 1"
author: "Frank Edwards"
date: '10/23/2019'
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(pander)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "tiny")
```

## Probability

**How often, on average, does an event occur? **

Probability is a set of tools for describing randomness. 

Probability helps us sort signal (patterns) from noise.

## Two core theories

*Frequentist:* Probability is the proportion of times an event occurs if we repeat an experiment under the same conditions many times

If $n_x$ is the number of heads, and $n_t$ is the number of coin flips, then the probability of heads is

$$P(x) \approx \frac{n_x}{n_t}$$
$$P(x) = \lim_{n_t\to\infty} \frac{n_x}{n_t}$$

*Bayesian:* Probability is a subjective judgment about the likelihood that an event occurs, with endpoints at 0 (never occurs) and 1 (always occurs)

## Definitions

*Deterministic* processes do not include randomness. For example, if I drop a ball, it will fall.

*Stochastic* events include randomness. For example, if I flip a fair coin, it will be heads half of the time.

Nearly all social processes are *stochastic*. 

## Definitions and axioms

- Experiment: an action that produces stochastic events
- Sample space ($\Omega$): a set of all possible outcomes of the experiment
- Event: a subset of the sample space

## Example: coin flips

- Experiment
    
    1. flip a coin
    2. roll a dice
    3. vote in democratic primary

- Sample space $\Omega$

    1. {Heads, Tails}
    2. {1,2,3,4,5,6}
    3. {abstain, vote for Harris, vote for Warren, vote for Sanders, vote for Castro}
    
- Event

    1. Heads, tails, not heads, heads or tails, heads and tails
    2. 3, even number, anything but 6
    3. Did not vote, voted for a woman, voted for a senator

## Probability with equal likelihood of events

If all outcomes are equally likely, and $n$ represents the number of elements in a given set, then probability $P$ of event $A$ is:

\[P(A) = \frac{n_A}{n_\Omega}\] 

## Probability axioms

1. The probability of any event A is non-negative: $P(A)\geq0$
2. The probability that one of the outcomes in the sample space occurs is 1: $P(\Omega)=1$
3. Addition rule: If events A and B are mutually exclusive: \[P(A \textnormal{ or } B) = P(A) + P(B)\]

## Probability that an event doesn't occur

$$1-P(\textnormal{not } A) = P(A)$$

If $P(\textnormal{rolling a 6}) = \frac{1}{6}$ then $P(\textnormal{not rolling a 6}) = \frac{5}{6}$

## Law of total probability

$$P(A) = P(A \textnormal{ and } B) + P(A \textnormal{ and not } B)$$

If $P(\textnormal{eats pizza}) = 0.5$ and $P(\textnormal{eats pizza, happy}) = 0.4$, then $P(\textnormal{eats pizza, unhappy})  = 0.1$

## General addition rule

$$P(A \textnormal{ or } B) = P(A) + P(B) - P(A \textnormal{ and } B)$$

If $P(\textnormal{happy})=0.5$, then $P(\textnormal{eats pizza or happy})=0.6$

## Permutations, combinations

How many ways can we arrange the set:{X,Y,Z}? \pause

XYZ, XZY, YXZ, YZX, ZXY, ZYX \pause

**Sampling without replacement**: how many permutations of $n$ elements are there when we draw $k$ at a time, and can't draw the same element twice 

\[_nP_k =\frac{n!}{(n-k)!}\]

Combinations are the number of selections without regard to their order

\[_nC_k=\frac{_nP_k}{k!}\]

## The birthday problem

How many people do we need to have in a class for there to be a 50 percent chance that at least two of them have the same birthday? 

Assume each date of birth is equally likely. 

\[P(\textnormal{two people have the same birthday}) = 1-P(\textnormal{nobody has the same birthday})\] 

## Non-unique birthdays as permutations

We have $k$ students

How many ways can $k$ birthdays be arranged if there are no duplicate birthdays? A sample without replacement tells us:

\[_nP_k =\frac{n!}{(n-k)!}\]

\[_{365}P_k = \frac{365!}{(365-k)!}\]

Sampling with replacement, $k$ non-unique birthdays can be arranged:

\[365^k\]

## As a probability problem

Because $P(A) = \frac{n_A}{n_\Omega}$, we can obtain $1-P(\textnormal{nobody has the same birthday})$ as the size of the set for non-unique combinations divided by the size of the sample space

\[_{365}P_k =1 - \frac{365!}{(365-k)!} \times \frac{1}{365^k}\]

## The results

```{r echo = FALSE}
birthday <- function(k) {
  logdenom <- k * log(365) + lfactorial(365 - k)
  lognumer <- lfactorial(365)
  pr <- 1 -   exp(lognumer - logdenom)
  pr
}

bday <- tibble(k = 1:50, pr = birthday(k))

ggplot(bday, aes(x = k, y = pr)) +
  geom_hline(yintercept = 0.5, colour = "white", size = 2) +
  geom_line() +
  geom_point() +
  scale_y_continuous(str_c("Probability that at least two",
                           "people have the same birthday", sep = "\n"),
                     limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
  labs(x = "Number of people")
```

## Simulation

Let's randomly draw $k$ birthdays *with replacement* to estimate how likely a shared birthday is for various class sizes. 

```{r}
sim_bdays <- function(k) {
  ## draw k random birthdays from the vector 1:365
  days <- sample(1:365, k, replace = TRUE)
  ## if there are no duplicates, there are k unique birthdays, return TRUE if duplicates
  length(unique(days)) < k
}

sim_bdays(1)
sim_bdays(366)
```

## Monte Carlo simulation

Repeat our random draw of $k$ birthdays a large number of times to approximate the solution. How likely is a shared birthday for 30 students?

```{r}
n<-100000
k<-30
sames<-rep(NA, n)
for(i in 1:n){
  sames[i]<-sim_bdays(k)
}
### monte carlo solution
mean(sames)
### exact solution
birthday(30)
```

## Monte Carlo to approximate a curve, 1000 simulations per k

```{r echo = FALSE}
k<-50
n<-1000
mc_results<-matrix(nrow=k, ncol=n)
mc_out<-data.frame(k =  1:50, result=NA)
for(i in 1:k){
  for(j in 1:n){
    mc_results[i,j]<-sim_bdays(i)
  }
  mc_out[i,"result"]<-mean(mc_results[i,])
}

ggplot(mc_out, aes(x = k, y = result)) +
  geom_hline(yintercept = 0.5, colour = "white", size = 2) +
  geom_line() +
  geom_point() +
  scale_y_continuous(str_c("Probability that at least two",
                           "people have the same birthday", sep = "\n"),
                     limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
  labs(x = "Number of people")
```

## Monte Carlo to approximate a curve, 10000 simulations per k

```{r echo = FALSE}
k<-50
n<-10000
mc_results<-matrix(nrow=k, ncol=n)
mc_out<-data.frame(k =  1:50, result=NA)
for(i in 1:k){
  for(j in 1:n){
    mc_results[i,j]<-sim_bdays(i)
  }
  mc_out[i,"result"]<-mean(mc_results[i,])
}

ggplot(mc_out, aes(x = k, y = result)) +
  geom_hline(yintercept = 0.5, colour = "white", size = 2) +
  geom_line() +
  geom_point() +
  scale_y_continuous(str_c("Probability that at least two",
                           "people have the same birthday", sep = "\n"),
                     limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
  labs(x = "Number of people")
```

## Joint probability

The joint probability of two events (A and B) occurring is expressed as

\[P(A \textnormal{ and } B)\]

## Marginal probability

The marginal probability of an event B is 

\[P(B)\]

## Conditional probability

The conditional probability of event $A$ occurring given that event $B$ occurred is the ratio of the joint probability of A and B divided by the marginal probability of B

\[P(A|B) = \frac{P(A \textnormal{ and } B)}{P(B)}\]

## Voter files

```{r}
data("FLVoters")
voters<-na.omit(FLVoters)
head(voters)
```

## Marginal probability

What is the probability that a random voter in the population is Black: $P(Black) = ?$

```{r}
voters %>% 
  count(race) %>% 
  mutate(n = n/sum(n))
```

Is a woman: $P(Woman)= ?$

```{r}
voters %>% 
  count(gender) %>% 
  mutate(n = n/sum(n))
```

## Joint probability

What is the probability that a voter is a Black woman: $P(\textnormal{Black and woman}) = ?$

```{r}
voters %>% 
  count(gender, race) %>% 
  mutate(n = n/sum(n)) %>% 
  pivot_wider(names_from = gender, values_from = n)
```

## What is the probability that a voter is a woman? 

```{r echo = FALSE}
voters %>% 
  count(gender, race) %>% 
  mutate(n = n/sum(n)) %>% 
  pivot_wider(names_from = gender, values_from = n)
```

Use the law of total probability:

\[P(A) = P(A \textnormal{ and } B) + P(A \textnormal{ and not } B)\]

put differently, for all categories of B $i$:

\[P(A) = \sum_{i=1}^n{P(A \textnormal{ and } B_i)}\]

## Conditional probability

If a voter is a man, what is the probability that he is Asian: $P(\textnormal{Asian|man})= ?$

```{r}
voters %>% 
  filter(gender=="m") %>% 
  count(race) %>% 
  mutate(n=n/sum(n))
```

## Conditional probability

Alternatively, we can use the definition of conditional probability as the ratio of the joint probability to the marginal probability: 

\[P(\textnormal{Asian|man}) = \frac{P(\textnormal{Asian and man})}{P(\textnormal{man})}\]

```{r}
voters %>% 
  count(gender, race) %>% 
  mutate(n = n/sum(n))%>% 
  pivot_wider(names_from = gender, values_from = n)
```

## Conditioning on more than one variable

What is the probability that a male voter over age 60 is white?

\[P(\textnormal{white|male and over 60)}\]

```{r}
voters %>% 
  mutate(over60=age>60) %>% 
  count(over60, gender, race) %>% 
  mutate(n=n/sum(n)) %>% 
  pivot_wider(names_from = gender, values_from = n)
```

## Conditioning on more than one variable

In general:

\[P(A \textnormal{ and }B|C) = \frac{P(A \textnormal{ and } B \textnormal{ and } C)}{P(C)}\]

and 

\[P(A|B \textnormal{ and }C) = \frac{P(A \textnormal{ and } B \textnormal{ and } C)}{P(B \textnormal{ and } C)}\]

## Independence

Two events are independent if knowledge of one event gives us no information about the other event. 

$P(A|B)=P(A)$ and $P(B|A)=P(B)$

\[ A \perp B \] 

if and only if 

\[P(A \textnormal{ and } B) = P(A)P(B)\]

## Are race and gender independent in voting population in Florida?

If independent, then we should observe $P(\textnormal{Black and male}) = P(\textnormal{Black})P(\textnormal{male})$ and so on for other groups.

```{r}
ind_test<-voters %>% 
  count(gender, race) %>% 
  mutate(n = n/sum(n)) %>% 
  pivot_wider(names_from = gender, values_from = n)

ind_test
```

## Are race and gender independent in voting population in Florida?

First, calculate the marginal probability of being male by summing over all joint probabilities for male by race

\[P(A) = \sum_{i=1}^n{P(A \textnormal{ and } B_i)}\]

```{r}
p_male<-sum(ind_test$m)
p_male
```

## Are race and gender independent in voting population in Florida?

Next, calculate the marginal probability for each racial group by summing over joint probabilities of sex by race

```{r}
p_race<-ind_test %>% 
  mutate(p_race = m+f)
p_race
```

## Are race and gender independent in voting population in Florida?

Now, examine whether the joint probability of sex and race is equal to the product of the marginal probability of being a man times the marginal probability of being in each racial group. 

```{r echo = FALSE, fig.height = 2.5, fig.width=4, fig.align="center"}
ggplot(p_race,
       aes(x = p_race * p_male,
           y = ind_test$m, color = race)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) +
  ylab(" P(race and male)") +
  xlab("P(race) * P(male)")
```

## Bayes' rule

Recall that a Bayesian perspective treats probability as a subjective opinion about how likely an event is. How should we change our beliefs after we make observations about the world?

Bayes' rule formalizes how we should update our beliefs based on evidence:

\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]

If we have a *prior* belief that event A has $P(A)$ chance of occurring, then we observe some data, represented as event $B$, we update our beliefs and obtain *posterior probability* $P(A|B)$. 

## Example: Detecting breast cancer

How good is a mammogram at detecting breast cancer? 

What we know: One percent of women have breast cancer. 80 percent of people who have cancer and take a mammogram test positive. 9.6 percent of people who take a mammogram get a positive result when they do not have breast cancer. 

If you take a mammogram and get a positive result, what is the probability that you have breast cancer?

## Rewriting as probabilities

\[P(\textnormal{Cancer}) = 0.01\]
\[P(\textnormal{Test positive|Cancer}) = 0.8\]
\[P(\textnormal{Test positive|No cancer}) = 0.096\]

Let cancer be event A, and testing positive be event B. 

We wish to know $P(A|B)$

## Using Bayes' rule

The prior probability of having cancer is 0.01. How should we update our belief that someone has cancer based on a positive test?

\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]

Using the law of total probability, we can rewrite the denominator of Bayes' rule as:

\[P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\textnormal{ not A})P(\textnormal{not }A)}\]

## Using Bayes' rule

We can apply Bayes' rule for A = Cancer, B = positive test:

\[P(\textnormal{Cancer|Test positive})=\] 

\[\frac{P(\textnormal{Test positive|Cancer})P(\textnormal{Cancer})} {P(\textnormal{Test positive|Cancer})P(\textnormal{Cancer}) +P(\textnormal{Test positive|No cancer})P(\textnormal{No cancer})}\]

\[P(\textnormal{Cancer|Test positive})=\frac{0.8 \times 0.01}{0.8 \times 0.01 +  0.096 \times 0.99}\]

```{r}
(0.8 *  0.01)/(0.8 *  0.01 + 0.096 * 0.99)
```

Given these probabilities, the posterior likelihood that someone has cancer given a prior probability of one percent and a positive test is about 0.078

## Homework, etc

- No class next week, ASC
- Homework: 6.6.1 questions 1-4. Try 5 and 6 if you like, but not required
- Lab today: brief mid-semester assessment