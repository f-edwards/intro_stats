---
title: "Discovery, 1"
author: "Frank Edwards"
date: '10/23/2019'
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(pander)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "tiny")
```

## Text data: setup

```{r size = "tiny"}
# Run me first
#install.packages(c("tm", "SnowballC, wordcloud, modelr"))
library(tm)
library(SnowballC)
library(wordcloud)
library(modelr)
```

## Load the federalist papers

```{r size = "tiny"}
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus.raw <- Corpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus.raw
```

## Federalist 1
```{r size = "tiny"}
content(corpus.raw[[1]])
```

## Tidy it up

```{r size = "tiny"}
corpus.prep<-tm_map(corpus.raw, content_transformer(tolower))
corpus.prep<-tm_map(corpus.prep, stripWhitespace)
corpus.prep<-tm_map(corpus.prep, removePunctuation)
corpus.prep<-tm_map(corpus.prep, removeNumbers)
```

## Federalist 1 (post processing)
```{r size = "tiny"}
content(corpus.prep[[1]])
```

## Remove stopwords
```{r size = "tiny"}
head(stopwords("english"))
corpus<-tm_map(corpus.prep, removeWords, stopwords("english"))
```

## Federalist 1 (post processing)
```{r size = "tiny"}
content(corpus[[1]])
```

## Stem words into comparable terms

```{r size = "tiny"}
corpus<-tm_map(corpus, stemDocument)
content(corpus[[1]])
```

## Format into document-term matrix

Obtain measures of term-frequency, how often a given stem-term appears in a document or corpus. 

```{r size = "tiny"}
dtm<-DocumentTermMatrix(corpus)
dtm.mat<-as.matrix(dtm)
dtm.mat[1:5,1:5]
```

## Topic discovery

Topic discovery is a form of exploratory data analysis that looks at the frequency of terms used in a text or corpus to identify topics discussed in documents. 

- Unsupervised learning approaches are agnostic, and look for patterns in the data without identified outcome or predictor variables. Examples: clustering, topic discovery
- Supervised learning uses theory or hypotheses to explore data looking for particular patterns across outcomes and predictors. Examples: regression

Assumptions: *texts are bags of words*

## Topic discovery: wordcloud for Federalist 12

```{r size = "tiny"}
wordcloud(colnames(dtm.mat), 
          dtm.mat[12,], 
          max.words = 20)
```

## Topic discovery: wordcloud for Federalist 24

```{r}
wordcloud(colnames(dtm.mat), 
          dtm.mat[24,], 
          max.words = 20)
```

## Adjust for common words in corpus

The term frequency-inverse document frequency penalizes terms that occur frequently across documents in the corpus, highlighting novel terms in particular texts.

\[\textrm{tf}-\textrm{idf}(w,d) = \textrm{tf}(w,d) \times \textrm{idf}(w) \]

\[\textrm{idf}(w) = log \left(\frac{n}{\textrm{df}\left(w \right)}\right) \]

## Weighted term frequency

Federalist 12

```{r}
dtm.tfidf<-as.matrix(weightTfIdf(dtm))
head(sort(dtm.tfidf[12,], decreasing=TRUE), n=10)
```

Federalist 24

```{r}
dtm.tfidf<-as.matrix(weightTfIdf(dtm))
head(sort(dtm.tfidf[24,], decreasing=TRUE), n=10)
```

## Remember the k-means algorithm?

![](./vis/kmeans.jpg)

## We can use k-means to group texts by theme

K-means is an unsupervised approach to uncovering groupings in the data. We can use it to learn and group texts by topic across the corpus.

```{r}
k<-4 # clusters
# hamilton authored papers (known)
hamilton<-c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
dtm.tfidf.hammy<-dtm.tfidf[hamilton,]
# k-means
km.out<-kmeans(dtm.tfidf.hammy, centers =k)
```

## Evaluating clusters

```{r}
for(i in 1:k){
  print("top words")
  print(head(sort(km.out$centers[i,], 
                  decreasing = TRUE),
             n=10))
  print(c("included texts in cluster", i))
  print(rownames(dtm.tfidf.hammy)[km.out$cluster==i])
}
```

## Predicting authorship of federalist papers

66 essays have a known author (Hamilton or Madison). Authorship of the remaining 11 is unknown. 

We will use writing style to predict who authored these 11 essays, based on word counts from the known-authorship essays.

```{r}
dtm1<-as.matrix(DocumentTermMatrix(corpus.prep))
### count words per 1,000
tfm<-dtm1/rowSums(dtm1)*1000
## subset to words of interest
words<-c("although", "always", "commonly", "consequently",
         "considerable", "enough", "there", "upon", "while", "whilst")
tfm<-tfm[,words]
```

## Calculate word usage by Hamilton, Madison

```{r}
madison<-c(10,14,37:48,58)
tfm.ave<-rbind(colSums(tfm[hamilton,]) / length(hamilton),
               colSums(tfm[madison,]) / length(madison))
tfm.ave
```

Hamilton likes there, upon. Madison likes whilst, consequently. We'll use these style differences to predict who wrote the unknown papers

## Predict authorship for unknown papers

Fit a regression model with authorship as the outcome for known papers using distinctive words for each author as predictor. Then predict authorship for unknown papers.

```{r}
author<-rep(NA, nrow(dtm1))
author[hamilton]<-TRUE
author[madison]<-FALSE
author_dat<-data.frame(author = author, tfm) %>% 
  filter(!(is.na(author)))
```

## Fit the model

Fit a model to known authorship papers

```{r}
m0<-lm(author~ upon + there + consequently + whilst,
       data = author_dat)
coef(m0)
```

Recall that upon, there are Hamilton's words, and consequently, whilst are Madison's. Regression coefficients line up there. 

## Model fit

How well does the model fit the data? 

- Evaluate total error of the model
  
  - Root Mean Square Error: $\sqrt{\frac{\sum_{i=1}^N(\hat{y_i}-y_i)^2}{N}}$
  - Coefficient of determination: $R^2 = 1-\frac{\sum_{i=1}^n(\hat{y_i}-y_i)^2}{\sum_{i=1}^n(y_i-\bar{y})^2}$

- Evaluate prediction performance

|                     | Positive, obs. | Negative, obs. |
|---------------------|--------------------|--------------------|
| Positive, pred. | **True positive**      | False positive     |
| Negative, pred. | False negative     | **True negative**      |

## How well did the model predict authors?

```{r}
author_dat %>% 
  mutate(yhat = fitted(m0)>0.5) %>% 
  group_by(author) %>% 
  summarise(prop.true.positive = mean(yhat==author))
```

Perfection!

However, we may have *overfit* the data. Fitting your model to a particular dataset is no guarantee that it will predict well *out-of-sample*

## Cross validation

We can hold out subsets of the data, re-fit the model, and check classification performance. This gives us a sense of how well our model performs at predicting new cases. 

**Leave-one-out cross validation: The algorithm**

For each row of the data $i \cdots n$
1. Remove $i$th row of the data
2. Fit the model to the remaining $n-1$ observations
3. Predict the outcome for the held-out $i$th observation, calculate prediction error

Then, calculate the mean prediction error across all n observations. 

## Cross validation using loops

See Arnold for a tidyverse approach using modelr

```{r}
n<-nrow(author_dat)
classify_out<-list()
for(i in 1:n){
  sub.fit<-lm(author~upon + there + consequently + whilst,
              data = author_dat[-i,])
  classify_out[[i]]<-predict(sub.fit, newdata = author_dat[i,])>0.5
  classify_out[[i]]<-classify_out[[i]]==author_dat$author[[i]]
}
classify_out<-unlist(classify_out)
mean(classify_out)
```

We still predict perfectly - this is a good model

## How about those unidentified cases?

```{r}
author<-rep(NA, nrow(dtm1))
author[hamilton]<-TRUE
author[madison]<-FALSE
author_dat<-data.frame(author = author, tfm) %>% 
  filter((is.na(author)))

yhat<-predict(m0, newdata = author_dat)
## Hamilton is >0.5
yhat<-yhat>0.5
table(yhat)
```

All but one attributed to Madison. 

## Lab, HW, next week

- Next week: network and spatial data
- Homework (two weeks on this one: due 10/30): Choose either text data, network data, or spatial data.
- Text data: 5.5.1, Network data 5.5.2, Spatial data 5.5.3
- Lab: more regression, review HW 4 and prediction from model