---
title: "Sampling and inference"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
# set global options
theme_set(theme_bw())
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "tiny")
```

# Large sample (asymptotic) theorems, point estimates, and uncertainty

## The law of large numbers

As a sample of draws from a random variable increases, the sample mean converges to the population mean $E(X)$

\[\bar{x}_n \rightarrow E(X) \]

## The law of large numbers: point estimates converge to population parameters as n increases

A Monte Carlo simulation where we draw from Binomial(10, 0.3) 1 time up to 1000 times, then compute $\bar{x}$

```{r echo = F, fig.height = 4}
## MC simulation, 1000 reps 
sims<-1000
## Take 1000 draws from binomial(0.3, 10)
x<-rbinom(sims, p = 0.3, size = 10)
### output df
out<-data.frame(n=1:sims, xbar = NA)
for(i in 1:sims){
  out$xbar[i]<-sum(x[1:i])/i
}
## or use xbar<-cumsum(x)/1:sims
ggplot(out, aes(y=xbar, x=n * 10)) +
  geom_line() +
  geom_abline(slope=0, intercept = 3, lty=2) + 
  coord_cartesian(ylim= c(1, 5)) + 
  labs(x = "Number of observations",
       y = expression(bar(x)))
```

## The Central Limit Theorem 

- If we draw independent random samples, as sample size $n$ increases, the distribution of the sample mean $\bar{x}$ approaches a Normal distribution.

## How many students eat pizza in a week?

We want to estimate $\bar{pizza}$, the proportion of students who eat pizza per week on campus. Our approach: randomly select 10 classrooms, then randomly select 10 students from each class. Count the number who ate pizza within the prior week (0 = No pizza, 1 = pizza)

## Monte Carlo simulations of a binomial variable p=0.7, n=10

1. Take 10 draws from $pizza \sim Binomial(10, 0.7)$
2. Compute $\bar{pizza}$
3. Repeat many times!

```{r echo = F}
### Binomial random variable, 10 observations, probability of success = 0.7
## simulate each sample size for 1000 replications
n<-10 # 10 students in a class
p<-0.7 # 70% chance of a 1
xbar_10<-rep(NA, 10)
xbar_100<-rep(NA, 100)
xbar_1000<-rep(NA, 1000)

for(i in 1:10){
  x_10<-rbinom(10, p=p, size=n)
  xbar_10[i]<-(mean(x_10))
}

for(i in 1:100){
  x_100<-rbinom(100, p=p, size=n)
  xbar_100[i]<-(mean(x_100))
}

for(i in 1:1000){
  x_1000<-rbinom(1000, p=p, size=n)
  xbar_1000[i]<-(mean(x_1000))
}

```

## 

```{r echo = FALSE}
ggplot(data.frame(xbar=xbar_10), 
       aes(x=xbar))+
  geom_histogram(aes(y = ..density..)) + 
  ggtitle("10 samples")
```

## 

```{r echo = FALSE}
ggplot(data.frame(xbar=xbar_100), 
       aes(x=xbar))+
  geom_histogram(aes(y = ..density..)) + 
  ggtitle("100 samples")
```

## 

```{r echo = FALSE}
ggplot(data.frame(xbar=xbar_1000), 
       aes(x=xbar))+
  geom_histogram(aes(y = ..density..)) + 
  ggtitle("1000 samples")
```

## Inference and the central limit theorem

Of course, we generally don't replicate 1000 times. 

Let's take 1 draw from the pizza study

```{r}
n<-10 # 10 students in a class
p<-0.7 # 70% chance of a 1
pizza<-rbinom(n = n, p = p, size = 10)
```

What can we say about the proportion of students who eat pizza at Rutgers?

## Inference and the central limit theorem

Our point estimate for the proportion is 

```{r}
mean(pizza)
```

Our standard deviation for the study is

```{r}
sd(pizza)
```

We can describe our estimate for the *sampling distribution* of $\bar{pizza}$ as a Normal distribution centered at `r mean(pizza)` with a standard error of `r round(sd(pizza)/sqrt(n),2)`. 

## Inference and the central limit theorem

We can construct a 95% confidence interval to describe how uncertain we are about the location of $\bar{pizza}$. Here, that interval is:

```{r}
## bounds = +/- 1.96 (Normal PDF for 95% mass)
se<-sd(pizza)/sqrt(10)
mean(pizza) + 1.96 * se
mean(pizza) - 1.96 * se
```

How should we interpret this interval? 

# Hypothesis testing

## The problem

- I think that the true proportion of students who eat pizza is 0.5
- How can I use my data to evaluate this claim?

## The basic approach

- Establish a hypothesis

1. $H_1: E[pizza] = 5$

- Evaluate how likely our observations are under the hypothesis

1. We know via CLT that $\bar{pizza} \sim N(\mu, \sigma^2)$
2. How likely is `r mean(pizza)` under a distribution with mean 5 and SE `r round(sd(pizza)/sqrt(10),2)`? 

## Our hypothesis for the sampling distribution of pizza habits

```{r echo = F}
ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 5, sd = sd(pizza)/sqrt(10))) + ylab("") +
  scale_y_continuous(breaks = NULL)
```

## What we observed

```{r echo = F}
ggplot(data = data.frame(x = c(0, 10)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 5, sd = sd(pizza)/sqrt(10))) + ylab("") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(aes(xintercept = mean(pizza)), lty = 2)
```

## How likely was our observation if H1 were true?

Use the Normal PDF to estimate

```{r}
pnorm(q = mean(pizza), mean = 5, sd = sd(pizza)/sqrt(10))
```

That's the proportion of observations that fall below our observation *if* $H_1$ is true. To convert this to how likely we are to observe our data *if* $H_1$ is true, subtract from 1

```{r}
1 - pnorm(mean(pizza), 5, sd(pizza)/sqrt(10))
```

What can we conclude? 

# Back to regression

## The linear regression model

\[ Y = \beta_0 + \beta_1 X + \varepsilon \] \pause

$\beta_0$: The value of $y$ when $x$ is equal to zero \pause

$\beta_1$: The average increase in $y$ when $x$ increases by one unit \pause

$\varepsilon$: The distance between the line $y = \beta_0 + \beta_1 X$ and the actual observed values of $y$. \pause

The line $E(y_i) = \beta_0 + \beta_1 x_i$ provides an expected value for $y_i$ based on the values of $x_i$.

## The linear regression model and prediction

Remember, that we put a $\hat{hat}$ on variables to indicate that they are estimated from the data, or predicted. \pause

In other words, we try to learn about the `true' *regression coefficients* $\beta_1$ and $\beta_0$ by estimating $\hat{\beta_1}$ and $\hat{\beta_0}$. \pause

## Standard errors of $\beta$

The standard error of $\beta$ is the standard deviation of its sampling distribution. \pause

In other words: $\hat{\beta} \sim N(\beta, SE_\beta^2)$ \pause

The standard error of $\beta$ is calculated as:

\[SE_\beta = \sqrt{\frac{\sum\varepsilon_i^2}{(n-2)\sum(x_i - \bar{x})^2}}\]

Note that the numerator captures variance in $y$ and the denominator captures variance in $x$

# Uncertainty and OLS

## The Mark of a Criminal Record

```{r}
### read and format Pager data
cr<-read_csv("https://raw.githubusercontent.com/f-edwards/intro_stats/master/data/criminalrecord.csv")
cr<-cr %>% 
  select(callback, crimrec)

head(cr)
```

## The Research question and the null hypothesis

- Does a criminal record make a callback less likely? \pause

- Implied null hypothesis: No difference in callback rates \pause

$H_0: E[Callback|Crimrec=T] - E[Callback|Crimrec = F] = 0$ \pause

Written differently: 

$H_0: E[Callback|Crimrec=T] = E[Callback|Crimrec = F]$

## Let's estimate the model for the effect of crimrec on callback

```{r}
library(broom)
m0<-lm(callback ~ crimrec, data = cr)
tidy(m0)
```

Write this out as a regression equation. 

- What does $\beta_0$ mean? 
- What does $\beta_1$ mean?

## Setting up our hypothesis test

$H_0:$ No effect of crimrec on callback. 

What does this imply in terms of $\beta$? 

Recall that our model says $E[callback] = \beta_0 + \beta_1Crimrec$

## Our null hypothesis for the central research question

$H_0: \beta_1 \sim N(0, SE^2_{\beta_1})$

What do we observe?

```{r}
tidy(m0)
```

## Computing the null hypothesis test manually

How likely is -0.125 if $H_0$ is true? 

Let's check our data against the Normal PDF for $H_0$

```{r}
pnorm(-0.125, 0, 0.0277)
```

What do we think?

# The Normal PDF and hypothesis testing

## The logic of a hypothesis test

Assume $H_0: Z(\beta) \sim N(0, 1)$ (standardized Beta follows a Z distribution)

We decide *a priori* that anything outside of the central 95% of the Normal PDF is inconsistent with $H_0$

```{r echo = F, fig.height = 4}
ggplot(data.frame(x=c(-5, 5)),
       aes(x=x)) + 
  stat_function(fun=dnorm, geom = "area", fill = "red",
                color = "black") + 
   stat_function(fun=dnorm,
                xlim= + 
                  qnorm(c(0.025, 0.975), 0, 1),
                geom="area",
                color = "black",
                fill="white")
```

## The logic of a hypothesis test

Now we observe our data and estimate our model. We find $\beta = 1.2$ and $SE = 0.8$. We convert that into a z-score $z = 1.2 / 0.8 = 1.5$ and check where it falls

```{r echo = F, fig.height = 4}
ggplot(data.frame(x=c(-5, 5)),
       aes(x=x)) + 
  stat_function(fun=dnorm, geom = "area", fill = "red",
                color = "black") + 
   stat_function(fun=dnorm,
                xlim= + 
                  qnorm(c(0.025, 0.975), 0, 1),
                geom="area",
                color = "black",
                fill="white") + 
  geom_vline(aes(xintercept = 1.5), lty = 2)
```

## The logic of a hypothesis test

Now we observe our data and estimate our model. We find $\beta = 1.2$ and $SE = 0.4$. We convert that into a z-score $z = 1.2 / 0.4 = 3$ and check where it falls

```{r echo = F, fig.height = 4}
ggplot(data.frame(x=c(-5, 5)),
       aes(x=x)) + 
  stat_function(fun=dnorm, geom = "area", fill = "red",
                color = "black") + 
   stat_function(fun=dnorm,
                xlim= + 
                  qnorm(c(0.025, 0.975), 0, 1),
                geom="area",
                color = "black",
                fill="white") + 
  geom_vline(aes(xintercept = 3), lty = 2)
```

## Null hypothesis testing: a recipe

1. Specify your null hypothesis (typically $Z(\beta) \sim N(0,1)$)
2. Specify your critical value (sometimes called $\alpha$), the threshold for finding 'statistical significance'
3. Estimate your model, compute a $z$ statistic for $\beta$
4. Compute whether your $Z(\beta)$ falls in the critical region of the $z$ distribution

## A note on t, z, and sample size

Technically, R will perform $t$ tests, not $z$ tests on our regression models. 

When our *degrees of freedom* are large, the $t$ distribution converges to the $z$ distribution (Normal(0,1)). 

*Degrees of freedom* for regression are defined as $n - k$, where $n$ is sample size, and $k$ is the number of parameters we are estimating in our model. 

## The t and the z: convergence for large DF

```{r echo = F}
ggplot(data.frame(x=c(-5, 5)), aes(x=x)) + 
  stat_function(fun=function(.x) dt(.x, df = 2),
                color = "red") +
  stat_function(fun=function(.x) dt(.x, df = 5),
                color = "blue") +
  stat_function(fun=function(.x) dt(.x, df = 30),
                color = "green") + 
  stat_function(fun=function(.x) dnorm(.x),
                color = "black") + 
  labs(subtitle = "t: Red df=2; Blue df=5; Green df=30; z:Black")

```

## Using the central limit theorem to calculate confidence intervals, compute p-values

If the sampling distribution for $\beta$ is defined as:

$\hat{\beta} \sim N(\beta, SE_\beta^2)$ \pause

Then we can construct a 95 percent CI for $\beta$ 

\[\hat{\beta} \pm 1.96 \times SE_\beta\] \pause

And conduct a $z$ test for $\hat{\beta}$ by evaluating how likely our estimated $\hat{\beta}$ is under the null hypothesis

$H_0: \beta \sim N(0,SE_\beta^2)$

## Using OLS to estimate the SATE

```{r}
cr_ols<-lm(callback ~ 
             crimrec,
           data = cr)

tidy(cr_ols)
```

- What is the implied null hypothesis here?
- How do we compute 'statistic' (z-statistic)?
- How do we compute 'p.value'?

## Interpretation

```{r}
tidy(cr_ols)
```

- What can we conclude about $\beta_1$? 
- What does this tell us about our focal research question?

## Statistical significance: an interpretation guide

Statistical significance give us information about whether differences we observe for some outcome across levels of a predictor in our data are likely to occur if there were no underlying differences in the data-generating process. \pause

Put simply: if there were no difference across levels, how likely would I be to observe what I did observe? \pause

What $t$ tests and $z$ tests do is provide a quick signal vs noise check for our data. 

## Statistical significance: CAUTIONS

Statistical significance testing is arbitrary. The null hypothesis is arbitrary, and 0.95 is arbitrary. \pause

DO NOT USE SIGNIFICANCE TESTING ALONE TO INFORM YOUR SCIENTIFIC JUDGMENT \pause

DO NOT USE SIGNIFICANCE TESTING ALONE TO DECIDE WHAT YOUR MODEL SHOULD BE 


