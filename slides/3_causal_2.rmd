---
title: "Causality, 2"
author: "Frank Edwards"
date: "9/17/2019"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

# Returning to Pager's experiment

## The causal effect

For observation $i$ is equal to callback_crimTRUE_i - callback_crimFALSE_i

*The fundamental problem of causal inference is that we only observe one of these outcomes*

## The counterfactual and potential outcomes

```{r echo= FALSE, message = FALSE}
dat<-read_csv("./data/criminalrecord.csv")
dat %>% 
  select(callback, crimrec) %>% 
  mutate(callback_crimTRUE = ifelse(crimrec==1,
                                    callback,
                                    NA),
         callback_crimFALSE = ifelse(crimrec==0,
                                     callback,
                                     NA)) %>% 
  select(crimrec, callback_crimTRUE, callback_crimFALSE)
```

## Randomized experiments (or RCTs)

- By randomizing assignment to treatment, we can treat units as equivalent
- If units are equivalent, we can estimate the average treatment effect as a difference in means on the outcome between the treatment and control group
- If we don't randomize, we have no assurance that the treated and control groups are equivalent, meaning we don't have a strong case that we've observed the counterfactual 

## Obtaining a sample average treatment effect

The sample average treatment effect is defined as:

\[ \textrm{SATE} = \frac{1}{n}\sum_{i=1}^n Y_i(1) - Y_i(0) \]

In practice, since we only observe $Y_i(1)$ OR $Y_i(0)$, we instead estimate a *difference-in-means* of the outcome between the treatment and control: $mean(Y(1)) - mean(Y(0))$. If assignment has been randomized, these values are identical.

# Why we randomize

## An experiment on voting and social pressure

```{r}
data(social)
glimpse(social)
```

## Obtaining mean voting by treatment/control

```{r}
control<-social %>% 
  filter(messages == "Control") %>% 
  summarise(primary2006 = mean(primary2006))

treatment<-social %>% 
  filter(messages!="Control") %>% 
  group_by(messages) %>% 
  summarise(primary2006 = mean(primary2006))

treatment
```

## The difference in means (causal effect)

```{r}
treatment %>% 
  mutate(effect = primary2006 - control$primary2006)
```

## Why randomization matters

```{r echo = FALSE}
prop_vot<-social %>% 
  group_by(yearofbirth) %>% 
  summarise(voting_prop = sum(primary2006)/n())

ggplot(prop_vot,
       aes(x = yearofbirth, y = voting_prop)) + 
  geom_point()
```

## Why randomization matters (continued)

```{r echo = FALSE}
sex_hh<-social %>% 
  group_by(sex, hhsize) %>% 
  summarise(voting_prop = sum(primary2006)/n())

ggplot(sex_hh,
       aes(x = hhsize, y = voting_prop, color = sex)) + 
  geom_point()

```

## Randomization matters 

- Because certain kinds of people are more likely to vote in primaries than others
- We note these differences between observed variables and our outcome: primary2006
- We didn't measure very much here. They could also differ across unobserved or unobservable variables!
- Randomization (given a large enough n) ensures that treatment and control groups are *identical* across all observed and unobserved/unobservable differences prior to treatment
- This condition -- statistically identical treatment and control groups -- is a necessary condition for causal inference. Randomization is the most straightforward way to achieve this condition.

# Causal inference in observational data

## Estimating the impact of a minimum wage increase

In 1992, New Jersey raised it's minimum wage from \$4.25 to \$5.05. Pennsylvania did not.

```{r}
data(minwage)
glimpse(minwage)
```

## Describing the data, categoricals

```{r}
table(minwage$chain)
table(minwage$location)
```

## Did NJ minimum wage increase the wages paid to employees?

```{r}
minwage %>% 
  group_by(location) %>% 
  summarise(wageBefore_mn = mean(wageBefore),
            wageAfter_mn = mean(wageAfter))
```

## Another way to look at change in wages

```{r}
minwage %>% 
  group_by(location) %>% 
  summarise(prop_below_before = mean(wageBefore>=5.05),
            prop_below_after = mean(wageAfter>=5.05))
```

## Look at our outcome variable

```{r}
minwage<-minwage %>% 
  mutate(prop_ft_pre = fullBefore / (fullBefore + partBefore),
         prop_ft_post = fullAfter / (fullAfter + partAfter)) 

minwage %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post))
```

## Assumption: PA is a no-treatment counterfactual

Estimate the causal effect 

```{r}
control<-minwage %>% 
  filter(location=="PA") %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

minwage %>% 
  filter(location!="PA") %>% 
  summarise(prop_ft_post = mean(prop_ft_post)) %>% 
  mutate(effect = prop_ft_post - control$prop_ft_post)
```

# Is this a valid estimate of the causal effect?

## Confounding jeopardizes causal inference

- Confounding bias: a third variable is associated with both the treatment and the outcome \pause
- Selection bias: a unit may choose to participate in a treatment for reasons that are correlated with the outcome \pause

**Correlation != Causation**

## Dealing with confounding

- Randomize treatment! \pause
- When we can't... \pause
- Statistical control: within-subgroup analysis based on confounder values \pause

## Are NJ and PA the same (at least when it comes to fast food jobs?)?  

```{r}
minwage %>% 
  group_by(location=="PA") %>% 
  summarise(prop_wendys = mean(chain=="wendys"),
            prop_bk = mean(chain=="burgerking"),
            prop_kfc = mean(chain=="kfc"),
            prop_roys = mean(chain=="roys"))
```

## Maybe restaurant chain matters? Let's control for it!

```{r}
control<-minwage %>% 
  filter(location=="PA") %>% group_by(chain) %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

minwage %>% 
  filter(location!="PA") %>% group_by(chain) %>% 
  summarise(prop_ft_post = mean(prop_ft_post)) %>% 
  mutate(effect = prop_ft_post - control$prop_ft_post)
```

## Maybe region matters: central and south vs north and shore

```{r}
control<-minwage %>% 
  filter(location=="PA") %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

minwage %>% 
  filter(location!="PA") %>% group_by(location) %>% 
  summarise(prop_ft_post = mean(prop_ft_post)) %>% 
  mutate(effect = prop_ft_post - control$prop_ft_post)
```

## Cross-sections and time series

- Longitudinal data: repeated measurements of the same unit on the same variables over time
- Cross-sectional data: one measurement of many units
- Panel data (or time series cross-sectional data): repeated measurements of many units on the same variables over time
- Key advantages to panel data: variables may differ across units and within-units over time (trends). 

## Before and after design

```{r echo = FALSE}
minwage_plot<-minwage %>% 
  ungroup() %>% 
  mutate(location = ifelse(location=="PA", "PA", "NJ")) %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post)) %>% 
  gather(time, full, -location) %>% 
  mutate(time = ifelse(time=="prop_ft_pre", 1, 2))

ggplot(minwage_plot,
       aes(x = time, y = full, color = location)) + 
  geom_point() + 
  geom_line()
```

## Difference in Differences

- What if we treated PA as the counterfactual, and used information about it's trend in employment to estimate the effect of NJ's minimum wage increase?
- Assumption: The trend in the outcome over time would have been identical across all units if the treatment had never been imposed (parallel trends)

## Difference in Differences (visual)

```{r echo = FALSE}
minwage_plot<-minwage %>% 
  ungroup() %>% 
  mutate(location = ifelse(location=="PA", "PA", "NJ")) %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post)) %>% 
  gather(time, full, -location) %>% 
  mutate(time = ifelse(time=="prop_ft_pre", 1, 2))

ggplot(minwage_plot,
       aes(x = time, y = full, color = location)) + 
  geom_point() + 
  geom_line() + 
  geom_segment(aes(x = 1, xend =2, y = 0.297, 
                   yend = 0.297 + (0.272-0.31)),
               lty =2, color = 2)
```

## Estimating the causal effect

Where $y_{ij}$ is the outcome for treatment group $i=1$ and post-treatment time $j=1$

\[ \textrm{DiD} = (\bar{y}_{1,1} - \bar{y}_{1,0}) - (\bar{y}_{2,1} -\bar{y}_{2,0})\]

Assuming that the counterfactual outcome for the treatment group has a parallel time trend to that observed for the control group.

## Compute the DiD estimator

```{r}
DiD<-minwage %>% 
  mutate(location = ifelse(location=="PA", "PA", "NJ")) %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post))

control <- DiD %>% filter(location=="PA")
treatment <- DiD %>% filter(location!="PA")
(treatment$prop_ft_post - treatment$prop_ft_pre) - 
  (control$prop_ft_post - control$prop_ft_pre)
```

# Descriptive Statistics

## Summarizing a variable

Reduce a vector to a single or smaller set of values that tell us something useful

Examples we've already used: 
- minimum: min() 
- maximum: max() 
- median: median() 
- mean: mean()

## Quantiles

- The median is the 0.5 quantile (50th percentile)
- Quantiles are less sensitive to outliers than are other measures (like the mean)
- Quantiles tell you the proportion of a data that falls below some cutpoint

## Quantiles: example

```{r}
quantile(minwage$wageBefore, 0.25)
quantile(minwage$wageBefore, 0.75)
quantile(minwage$wageBefore, c(0.05, 0.25, 0.5, .75, 0.95))

```

## Standard deviation

- The standard deviation (SD, $\sigma$) is a measure of the spread of a variable
- It provides a measure of how much each observation of a variable differs from the mean of the variable
- You can use the sd() function in R
- The variance (var() function) is the square of the standard deviation

$$\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x}^2)}$$ 
$$variance = \sigma^2$$

## Calculate an SD for these variables
```{r}
minwage$wageBefore[1:10]
minwage$fullBefore[1:10]
```

## Homework

- Complete exercise 2.8.1
- load the data with data(STAR)
- make sure to use na.rm = TRUE for mean(), quantile() and other functions
- Recode variables to character rather than factor types using case_when() or ifelse()
- You will use group_by() and summarise() alot on this assignment
- Don't use View(), use head() - this is a 6000 row dataset. 