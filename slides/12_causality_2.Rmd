---
title: "Causality, observational studies"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(gridExtra)
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "small")
```

## An experiment on voting and a social pressure 

*Civic duty:* The whole point of democracy is that citizens are active participants in government; that we have a voice in government. Your voice starts with your vote. On August 8, remember your rights and responsibilities as a citizen. Remember to vote. DO YOUR CIVIC DUTY – VOTE \pause

*Hawthorne effect (surveillance):* This year, we’re trying to figure out why people do or do not vote. We’ll be studying voter turnout in the August 8 primary election. Our analysis will be based on public records, so you will not be contacted again or disturbed in any way. Anything we learn about your voting or not voting will remain confidential and will not be disclosed to anyone else. DO YOUR CIVIC DUTY – VOTE

## An experiment on voting and a social pressure 

```{r size = "tiny"}
social <- read_csv("https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv")
head(social)
```

## Obtaining mean voting by treatment/control

```{r size = "tiny"}
control <- social %>% 
  filter(messages == "Control") %>% 
  summarise(primary2006 = mean(primary2006))

treatment <- social %>% 
  filter(messages != "Control") %>% 
  summarise(primary2006 = mean(primary2006))

control
treatment
```

## The difference in means (causal effect)

```{r}
effect <- treatment - control

effect
```

## Why randomization matters

```{r echo = T, fig.height = 3, size = "tiny"}
prop_vot<-social %>% 
  group_by(yearofbirth) %>% 
  summarise(voting_prop = sum(primary2006)/n())

ggplot(prop_vot,
       aes(x = yearofbirth, y = voting_prop)) + 
  geom_point() + 
  ylab("Proportion voting in 2006 primary") + 
  xlab("Year of birth")

```

## Why randomization matters (continued)

```{r echo = T, fig.height = 3, size = "tiny"}
sex_hh<-social %>% 
  group_by(sex, hhsize) %>% 
  summarise(voting_prop = sum(primary2006)/n())

ggplot(sex_hh,
       aes(x = hhsize, y = voting_prop, color = sex)) + 
  geom_point() + 
  xlab("Household size") + 
  ylab("Proportion voting in 2006 primary") 

```

## Randomization matters 

- Because certain kinds of people are more likely to vote in primaries than others \pause
- We note these differences between observed variables and our outcome: primary2006 \pause
- We didn't measure very much here. They could also differ across unobserved or unobservable variables! \pause
- Randomization (given a large enough n) ensures that treatment and control groups are *identical* across all observed and unobserved/unobservable differences prior to treatment \pause
- This condition -- statistically identical treatment and control groups -- is a necessary condition for causal inference. Randomization is the most straightforward way to achieve this condition.

# Causal inference in observational data

## Estimating the impact of a minimum wage increase

In 1992, New Jersey raised it's minimum wage from \$4.25 to \$5.05. Pennsylvania did not.

```{r size = "tiny"}
minwage <- read_csv("https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/minwage.csv")
head(minwage)
```

## Describing the data, categoricals

```{r size = "tiny"}
table(minwage$chain)
table(minwage$location)
```

## Did NJ minimum wage increase the wages paid to employees?

```{r size = "tiny"}
minwage %>% 
  group_by(location) %>% 
  summarise(wageBefore_mn = mean(wageBefore),
            wageAfter_mn = mean(wageAfter))
```

## Another way to look at change in wages

```{r}
minwage %>% 
  group_by(location) %>% 
  summarise(prop_below_before = mean(wageBefore>=5.05),
            prop_below_after = mean(wageAfter>=5.05))
```

## Look at our outcome variable

```{r}
###Compute proportion full time before
###And after

minwage<- minwage %>% 
  mutate(prop_ft_pre = 
           fullBefore / 
           (fullBefore + partBefore))

minwage <- minwage %>% 
  mutate(prop_ft_post = 
           fullAfter / 
           (fullAfter + partAfter))
```

## Look at our outcome variable

```{r size = "tiny"}
minwage %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post))
```

## Assumption: PA is a no-treatment counterfactual

Estimate the causal effect 

```{r size = "tiny"}
control <- minwage %>% 
  filter(location == "PA") %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

treatment <- minwage %>% 
  filter(location != "PA") %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

treatment - control
```

# Is this a valid estimate of the causal effect?

## Confounding jeopardizes causal inference

- Confounding bias: a third variable is associated with both the treatment and the outcome \pause
- Selection bias: a unit may choose to participate in a treatment for reasons that are correlated with the outcome \pause

**Correlation != Causation**

## Dealing with confounding

- Randomize treatment! \pause
- When we can't... \pause
- Statistical control: within-subgroup analysis based on confounder values 

## Are NJ and PA the same (at least when it comes to fast food jobs?)?  

```{r}
minwage %>% 
  group_by(location) %>% 
  summarise(prop_wendys = mean(chain=="wendys"),
            prop_bk = mean(chain=="burgerking"),
            prop_kfc = mean(chain=="kfc"),
            prop_roys = mean(chain=="roys"))
```

## Maybe restaurant chain matters? Let's control for it!

```{r}
control<-minwage %>% 
  filter(location=="PA") %>% 
  group_by(chain) %>% 
  summarise(prop_ft_post = mean(prop_ft_post))
```

## Maybe restaurant chain matters? Let's control for it!

```{r size = "tiny"}
treatment<-minwage %>% 
  filter(location!="PA") %>% 
  group_by(chain) %>% 
  summarise(prop_ft_post = mean(prop_ft_post)) 
```


## Maybe restaurant chain matters? Let's control for it!

```{r}
treatment$effect<-treatment$prop_ft_post - 
  control$prop_ft_post

treatment
```

## Maybe region matters: central and south vs north and shore

```{r size = "tiny"}
control<-minwage %>% 
  filter(location=="PA") %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

treatment<-minwage %>% 
  filter(location!="PA") %>% 
  group_by(location) %>% 
  summarise(prop_ft_post = mean(prop_ft_post))

control

treatment
```

## Maybe region matters?

```{r}
treatment$effect<-treatment$prop_ft_post - 
  control$prop_ft_post

treatment
```


## Cross-sections and time series

- Longitudinal data: repeated measurements of the same unit on the same variables over time \pause
- Cross-sectional data: one measurement of many units \pause
- Panel data (or time series cross-sectional data): repeated measurements of many units on the same variables over time \pause
- Key advantages to panel data: variables may differ across units and within-units over time (trends). 

## Before and after design (longitudinal)

```{r echo = FALSE}
minwage_plot<-minwage %>% 
  ungroup() %>% 
  mutate(location = ifelse(location=="PA", "PA", "NJ")) %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post)) %>% 
  gather(time, full, -location) %>% 
  mutate(time = ifelse(time=="prop_ft_pre", 1, 2))

ggplot(minwage_plot,
       aes(x = time, y = full, color = location)) + 
  geom_point() + 
  geom_line()
```

## Difference in Differences

- What if we treated PA as the counterfactual, and used information about it's trend in employment to estimate the effect of NJ's minimum wage increase? \pause
- Assumption: The trend in the outcome over time would have been identical across all units if the treatment had never been imposed (parallel trends)

## Difference in Differences (visual)

```{r echo = FALSE}
minwage_plot<-minwage %>% 
  ungroup() %>% 
  mutate(location = ifelse(location=="PA", "PA", "NJ")) %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post)) %>% 
  gather(time, full, -location) %>% 
  mutate(time = ifelse(time=="prop_ft_pre", 1, 2))

ggplot(minwage_plot,
       aes(x = time, y = full, color = location)) + 
  geom_point() + 
  geom_line() + 
  geom_segment(aes(x = 1, xend =2, y = 0.297, 
                   yend = 0.297 + (0.272-0.31)),
               lty =2, color = 2)
```

## Estimating the causal effect: Difference in Differences

Where $y_{ij}$ is the outcome for treatment group $i=1$ and post-treatment time $j=1$

\[ \textrm{DiD} = (\bar{y}_{1,1} - \bar{y}_{1,0}) - (\bar{y}_{2,1} -\bar{y}_{2,0})\]

Assuming that the counterfactual outcome for the treatment group has a parallel time trend to that observed for the control group.

## Compute the DiD estimator

```{r size = "tiny", echo = F}
minwage %>% 
  mutate(location = 
           ifelse(location=="PA", "PA", "NJ")) %>% 
  group_by(location) %>% 
  summarise(prop_ft_pre = mean(prop_ft_pre),
            prop_ft_post = mean(prop_ft_post))
```
\pause

\[ \textrm{DiD} = (\bar{y}_{1,1} - \bar{y}_{1,0}) - (\bar{y}_{2,1} -\bar{y}_{2,0})\] \pause

```{r}
### the DiD Estimator
(0.320 - 0.297) - (0.272 - 0.310)
```

# Descriptive Statistics

## Summarizing a variable

Reduce a vector to a single or smaller set of values that tell us something useful

Examples we've already used: 
- minimum: min() 
- maximum: max() 
- median: median() 
- mean: mean()

## Quantiles

- The median is the 0.5 quantile (50th percentile)
- Quantiles are less sensitive to outliers than are other measures (like the mean)
- Quantiles tell you the proportion of a data that falls below some cutpoint

## Quantiles: example

```{r}
quantile(minwage$wageBefore, 0.25)
```

## Quantiles: example

```{r}
quantile(minwage$wageBefore, 0.75)
```


## Quantiles: example

```{r}
quantile(minwage$wageBefore, c(0.05, 0.25, 0.5, .75, 0.95))
```

## Standard deviation

- The standard deviation (SD, $\sigma$) is a measure of the spread of a variable \pause
- It provides a measure of how much each observation of a variable differs from the mean of the variable \pause
- You can use the sd() function in R \pause
- The variance (var() function) is the square of the standard deviation

$$\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}$$ \pause
$$variance = \sigma^2$$

## Compute an SD for these variables

```{r}
minwage$wageBefore[1:10]
minwage$fullBefore[1:10]
```

## Standard deviations and meaningful differences

How meaningful is a ten point difference on a test?

```{r, size = "tiny"}
### draw 50 random scores from a test with a minimum of zero and maximum of 100
testA<-runif(50, 0, 100)
### draw 50 random scores from a test with a minimum of zero and maximum of 1000
testB<-runif(50, 0, 1000)
```

## Visualizing the distributions

```{r include = F}
tests<-data.frame(score = c(testA, testB), test = rep(c("A", "B"), each = 50))
ggplot(tests,
       aes(x = score)) + 
  geom_histogram() + 
  facet_wrap(~test, scales = "free") 
```


## The mean

```{r include = F}
p1<-ggplot(tests %>% 
             filter(test == "A"),
       aes(x = score)) + 
  geom_histogram() + 
  geom_vline(xintercept = 50, lty = 2) 

p2<-ggplot(tests %>% 
             filter(test == "B"),
       aes(x = score)) + 
  geom_histogram() + 
  geom_vline(xintercept = 500, lty = 2) 

grid.arrange(p1, p2)
```

## A 10 point jump from the mean

```{r include = F}

p1<-ggplot(tests %>% 
             filter(test == "A"),
       aes(x = score)) + 
  geom_histogram() + 
  geom_vline(xintercept = 50, lty = 2) +
  geom_vline(xintercept = 50 + 10, lty = 2, color = "red") 


p2<-ggplot(tests %>% 
             filter(test == "B"),
       aes(x = score)) + 
  geom_histogram() + 
  geom_vline(xintercept = 500, lty = 2) +
  geom_vline(xintercept = 500 + 10, lty = 2, color = "red") 


grid.arrange(p1, p2)
```

## A 1 SD jump from the mean

```{r include = F}
p1<-ggplot(tests %>% 
             filter(test == "A"),
       aes(x = score)) + 
  geom_histogram() + 
  geom_vline(xintercept = 50, lty = 2) +
  geom_vline(xintercept = 50 + sd(testA), lty = 2, color = "red") 


p2<-ggplot(tests %>% 
             filter(test == "B"),
       aes(x = score)) + 
  geom_histogram() + 
  geom_vline(xintercept = 500, lty = 2) +
  geom_vline(xintercept = 500 + sd(testB), lty = 2, color = "red") 


grid.arrange(p1, p2)
```

## Homework

- HW4 posted to Slack
- make sure to use `na.rm = TRUE` for mean(), quantile() and other functions
- `group_by()` and `summarize()` are very helpful on this one
