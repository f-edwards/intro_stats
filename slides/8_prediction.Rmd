---
title: "Prediction, 1"
author: "Frank Edwards"
date: "11/2/21"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(MASS)
select<-dplyr::select
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "small")
```

# Prediction

## Why predict?

- To learn about what is likely to happen in the future (weather, elections, economic changes) \pause
- To validate theories or arguments:
  - Valid causal inference requires successful prediction of counterfactual claims \pause
  - e.g. if X were different, what value of Y would we observe?
  
## Load and process polls data

```{r size = "tiny"}
polls<-read.csv("https://raw.githubusercontent.com/f-edwards/intro_stats/master/data/polls2016.csv")

results<-read.csv("https://raw.githubusercontent.com/f-edwards/intro_stats/master/data/1976-2016-president.csv")
```


## How can we join these two data frames? 

- Harmonize the data (consistent column names, variable labels, units of analysis) \pause

```{r size = "tiny"}
polls<-polls %>% 
  filter(population == "Likely Voters") %>% 
  select(state, electoral_votes, Clinton, Trump) %>% 
  pivot_longer(cols = Clinton:Trump,
               names_to = "candidate", 
               values_to = "poll_result")

results<-results %>% 
  filter(year==2016, 
         office == "US President",
         party == "democrat" | party == "republican") %>% 
  mutate(candidate = case_when(
    candidate == "Clinton, Hillary" ~ "Clinton",
    candidate == "Trump, Donald J." ~ "Trump"
  )) %>% 
  select(state_po, candidate, candidatevotes, totalvotes) %>% 
  rename(state = state_po)
```

## How can we join these two data frames?

- Join the data frames
- create needed variables for analysis

```{r}
polls_results<-polls %>% 
  left_join(results) %>% 
  mutate(pct_vote = candidatevotes / totalvotes * 100)
```


## Calculate prediction error

Error is a general term for how wrong our guess is. We can generally calculate error by subtracting the observation from our prediction. \pause

`prediction error` = `predicted value` - `observed value` \pause

```{r size = "tiny"}
polls_results<-polls_results %>% 
  mutate(error = poll_result - pct_vote)

head(polls_results)
```

## Evaluate the errors

```{r}
polls_results %>% 
  group_by(candidate) %>% 
  summarise(mean_error = mean(error))
```

## Evaluate the errors

```{r size = "tiny", fig.height = 3}
ggplot(polls_results,
       aes(x = error)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = 0), lty=2) +
  facet_wrap(~candidate, ncol = 1)
```

## Root Mean Square Error

RMSE provides a measure of absolute error, where positive and negative errors don't negate each other

$$RMSE = \sqrt{\frac{\sum_{i=1}^n(\hat{y} - y)^2}{n{}}}$$

```{r}
polls_results %>% 
  group_by(candidate) %>% 
  summarise(rmse = sqrt(mean(error^2)))
```

## Conclusions on errors

1. Polls had similar magnitude of error for both candidates (RMSE)
2. Poll errors were consistently negative for Trump, were zero on average for Clinton. 

## Classification and prediction

How many polls called it right?

1. Make an average prediction for each state across polls
2. Whichever candidate has the highest average polling number is predicted the winner

## Making a prediction based on the polls

```{r size = "scriptsize"}
poll_winner<-polls_results %>% 
  group_by(state, candidate) %>% 
  summarise(poll_mean = mean(poll_result)) %>% 
  filter(poll_mean == max(poll_mean)) 

# group_by with filter will perform the filter operation
# over each unit in the group (states)

table(poll_winner$candidate)
```

## What percent of electoral college votes does our prediction yield for Clinton

```{r size =  "tiny"}
poll_winner %>% 
  left_join(polls %>% 
              select(state, electoral_votes) %>% 
              distinct()) %>% 
  group_by(candidate) %>% 
  summarise(electoral_pct_polls = sum(electoral_votes) / 538 * 100)


## actual result for Clinton
227/538 * 100
```

## Classification: potential outcomes for binary predictions

**Bold** cells are correct classifications. 

|                     | Positive, obs. | Negative, obs. |
|---------------------|--------------------|--------------------|
| Positive, pred. | **True positive**      | False positive     |
| Negative, pred. | False negative     | **True negative**      |

## Check our performance

- First, format the data to join the election winner onto our poll result predictions

```{r size = "tiny"}
poll_winner<-poll_winner %>% 
  select(state, candidate) %>% 
  rename(poll_winner = candidate)

election_winner<-results %>% 
  group_by(state) %>% 
  filter(candidatevotes == max(candidatevotes)) %>% 
  rename(election_winner = candidate) %>% 
  select(state, election_winner)

head(election_winner)
head(poll_winner)
```

## Join the data frames

```{r}
poll_performance<-poll_winner %>% 
  left_join(election_winner)

head(poll_performance)
```

## Then make correct classification a binary outcome

```{r}
polls_performance<-poll_performance%>% 
  mutate(poll_correct = poll_winner == election_winner)
```

## How often were the polls right?

```{r}
## calculate proportion of accurate classifications
## i.e. clinton_wins_pred == clinton_wins_vote

mean(polls_performance$poll_correct)
```

## Which ones did they get wrong?

```{r}
## Get misclassifications
polls_performance %>% 
  filter(!poll_correct)
```

# Linear regression

## Linear regression: IPV data

```{r size = "tiny"}
ipv<-read.csv("https://raw.githubusercontent.com/f-edwards/intro_stats/master/data/dhs_ipv.csv")

head(ipv)
```

## Research question

- Are secondary school completion rates for women associated with lower levels of acceptance of intimate partner violence? \pause
- *Hypothesis:* Feminist theory suggests a negative association between schooling and tolerance for intimate partner violence. In places where women have more social and economic power, tolerance for intimate partner violence should be lower.

## Visualizing associations: scatterplots

```{r size = "tiny", fig.height = 4}
ggplot(ipv, 
       aes(x = sec_school, y = beat_goesout)) +
  geom_point()
```

## Describing linear associations: correlation

```{r size = "tiny", fig.height = 4}
cor(ipv$sec_school, ipv$beat_goesout, use = "complete")
ggplot(ipv, 
       aes(x = sec_school, y = beat_goesout)) +
  geom_point() + 
  geom_smooth(method = "lm", se = F)
```

## Limits of correlation coefficients and importance of visualization

```{r echo=FALSE, fig.height =4, size = "tiny"}
### prep anscombe for small multiple plotting
### need a long dataframe with x,y,varname columns
data(anscombe)
plot_anscombe<-bind_cols(
  anscombe%>%
    select(x1,x2,x3,x4)%>%
    gather()%>%
    rename(x = value)%>%
    select(x),
  anscombe%>%
    select(y1,y2,y3,y4)%>%
    gather()%>%
    rename(y = value,
           var = key))
  
ggplot(plot_anscombe, 
       aes(x = x, y = y)) + 
  geom_smooth(method = "lm", 
              formula = y~ x,
              se = F) + 
  geom_point() + 
  facet_wrap(~var, scales = "free")

plot_anscombe %>% 
  group_by(var) %>% 
  summarise(cor = cor(x,y))
```

## Limits of linear relationships (continued)

```{r echo = FALSE}
r<-0.95
sample <- mvrnorm(n=1000, 
                  mu=c(0, 0), 
                  Sigma=matrix(c(1, r, r, 1), nrow=2), 
                  empirical=TRUE)

sample<-data.frame(x = sample[,1], y = sample[,2])

sample$y<-sample$y^2

ggplot(sample, 
       aes(x = x, y =y)) +
  geom_point() + 
  geom_smooth(method = "lm", se = F) +
  labs(title = paste("cor(x,y)=",round(cor(sample$x, sample$y),3)))
```

## Correlations and linear relationships

- A correlation coefficient ranges between [-1,1] \pause
- A correlation coefficient of 1 or -1 indicates a perfect linear association: x=y (if x and y are SD scaled) \pause
- A positive correlation coefficient indicates a positive slope \pause
- A negative correlation coefficient indicates a negative slope \pause
- A weak correlation does not imply that there is no relationship

## Lines

We can define a line as:

\[y = mx + b\]

Where $m$ is the slope and $b$ is the y-intercept.


## Lines: slopes

What will the line $y = x + 2$ look like? \pause

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = 2)
```

## Lines: slopes

What will the line $y = 2x + 2$ look like? 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 2, intercept = 2)
```


## Lines: slopes

What will the line $y = -2x + 2$ look like? 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = -3, intercept = 2)
```

## Lines: slopes

What will the line $y = 0.2x + 2$ look like? 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 0.2, intercept = 2)
```

## Lines: intercepts

What will the line $y = x + 0$ look like? 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = 0)
```

## Lines: intercepts

What will the line $y = x + 2$ look like? 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = 2)
```

## Lines: intercepts

What will the line $y = x - 3$ look like? 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = -3)
```

## The linear regression model

We can describe the relationship between a predictor variable $X$ and an outcome variable $Y$ with a line:

\[y = mx + b\] 

What does m describe? \pause

- The increase in y for a one-unit increase in x

What does b describe \pause

- The location of y when x = 0


## The linear regression model: expected value

We can describe the relationship between a predictor variable $X$ and the expected value $E$ of an outcome variable $Y$ with the line:

\[ E[Y] = \beta_0 + \beta_1 X  \] \pause

What does $\beta_0$ describe? \pause

What does $\beta_1$ describe?

## The error term in linear regression

We can describe the relationship between a predictor variable $X$ and an outcome variable $Y$ with the line:

\[ Y = \beta_0 + \beta_1 X + \varepsilon \]

Where $\beta_0$ is the y-intercept of the line, $\beta_1$ is the slope of the line, and $\varepsilon$ is the error between the fitted line and the coordinates $(X,Y)$

## The linear regression model

\[ Y = \beta_0 + \beta_1 X + \varepsilon \]

$\beta_0$: The value of $y$ when $x$ is equal to zero \pause

$\beta_1$: The average increase in $y$ when $x$ increases by one unit \pause

$\varepsilon$: The distance between the line $y = \beta_0 + \beta_1 X$ and the actual observed values of $y$. Allows us to estimate the line, even when x and y do not fall exactly on a line. \pause

The line $y = \beta_0 + \beta_1 X$ provides a prediction for the values of $y$ based on the values of $x$.

## The linear regresion model as a prediction engine

The line $y_i = \beta_0 + \beta_1 x_i$ provides a prediction for the value $y_i$ based on the value of $x_i$. \pause

- If $\beta_0 = 2$ and $\beta_1 = 1.5$, what is the expected value of $y$ when $x = 4$? \pause

- When $x = 2$?

## The linear regression model and prediction

We put a $\hat{hat}$ on variables to indicate that they are estimated from the data, or predicted.

A regression line predicts values $Y$, $\hat{Y}$ with the equation:

$\hat{Y} = \beta_0 + \beta_1 X$

and the residual, or prediction error is the difference between the observed and predicted values of $Y$

$\varepsilon = Y_{obs} - \hat{Y}$

## Understanding the regression line for real data

```{r echo = FALSE, size = "tiny"}
r<-0.95
sample <- mvrnorm(n=10, 
                  mu=c(1, 1), 
                  Sigma=matrix(c(1, r, r, 1), nrow=2), 
                  empirical=TRUE)

sample<-data.frame(x = sample[,1], y = sample[,2])

m1<-lm(y~x, data = sample)
coefs<-coef(m1)

as_tibble(sample)
```

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

- Estimate $\hat{Y}$. Recall that $\hat{Y} = \beta_0 + \beta_1 X$ \pause
- Estimate $\varepsilon$. Recall that $\varepsilon = Y - \hat{Y}$

## Understanding the regression line

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() #+ 
  #geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  #geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding the fit

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) #+ 
  #geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding $\hat{y}$

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding $\varepsilon$

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2, size = 2)  +
  geom_segment(aes(x = x, xend =x, y = y, yend = yhat), lty =2)
```

## Ordinary least squares

We usually fit a linear regression using a method called *ordinary least squares*, or OLS. \pause

- This method seeks to minimize the distance between $\hat{Y}$ and $Y$. \pause
- To do so, we minimize the sum of squared residuals (SSR) \pause

In other words, we solve for the values of $\beta_0$ and $\beta_1$ that results in the smallest possible value for:

\[ \textrm{SSR} = \sum_{i=1}^n \varepsilon_i^2 =  \sum_{i=1}^n (y_i - \beta_0 - \beta_1 X )^2\] \pause

Also note that we can estimate the coefficient vector $\beta_1$ using matrix algebra:

\[\beta= (X^{\rm T} X )^{-1} X^{\rm T} Y \]

See Imai for a more details on the math behind OLS

## Estimating a regression model in R, the basics

```{r fig.height = 2, size = "tiny"}
x<-c(1, 2, 3, 4, 5)
y<-c(2, 5, 1, 8, 10)

model_demo<-lm(y~x)

coef(model_demo)
```

## The observed data

```{r fig.height = 2, fig.width = 3, size = "tiny"}
ggplot(data.frame(x = x, y = y),
       aes(x = x, y = y)) + 
  geom_point()
```

## The regression line

```{r fig.height = 2, , fig.width = 3, size = "tiny"}
ggplot(data.frame(x = x, y = y),
       aes(x = x, y = y)) + 
  geom_point() + 
  geom_abline(intercept = coef(model_demo)[1], 
              slope = coef(model_demo)[2])
```

## Asking a question with regression

*Are secondary school completion rates for women associated with lower levels of acceptance of intimate partner violence?*  \pause

- What is the outcome variable (y)? What is the predictor variable (x)? \pause

E[acceptance of intimate partner violence] = Intercept + Slope $\times$ secondary school completion \pause

- What is our implied prediction about the slope? 

## Estimating a regression model in R

```{r}
## models take the general form
## lm(outcome ~ predictor, data)
ipv_model<-lm(beat_goesout ~ sec_school, 
              data = ipv)

coef(ipv_model)
```

- What does the intercept coefficient ($\beta_0$) indicate? \pause
- What does the slope coefficient ($\beta_1$) indicate?

## Visualize the model

```{r echo = F}
ggplot(ipv %>% 
         filter(!(is.na(sec_school)), !(is.na(beat_goesout))), 
       aes(x=sec_school, y = beat_goesout)) + 
  geom_point() + 
  geom_abline(aes(intercept = coef(ipv_model)[1], 
                  slope = coef(ipv_model)[2])) #+ 
  # geom_point(aes(x = sec_school, y = fitted(ipv_model)), color = "red") + 
  # geom_segment(aes(x = sec_school, xend = sec_school, 
  #                  y = beat_goesout, yend = fitted(ipv_model)), lty =2)
```

## Visualize the model: expected values of y

```{r echo = F}
ggplot(ipv %>% 
         filter(!(is.na(sec_school)), !(is.na(beat_goesout))), 
       aes(x=sec_school, y = beat_goesout)) + 
  geom_point() + 
  geom_abline(aes(intercept = coef(ipv_model)[1], 
                  slope = coef(ipv_model)[2])) + 
  geom_point(aes(x = sec_school, y = fitted(ipv_model)), color = "red") #+ 
  # geom_segment(aes(x = sec_school, xend = sec_school, 
  #                  y = beat_goesout, yend = fitted(ipv_model)), lty =2)
```

## Visualize the model: error term (residuals)

```{r echo = F}
ggplot(ipv %>% 
         filter(!(is.na(sec_school)), !(is.na(beat_goesout))), 
       aes(x=sec_school, y = beat_goesout)) + 
  geom_point() + 
  geom_abline(aes(intercept = coef(ipv_model)[1], 
                  slope = coef(ipv_model)[2])) + 
  geom_point(aes(x = sec_school, y = fitted(ipv_model)), color = "red") + 
  geom_segment(aes(x = sec_school, xend = sec_school,
                   y = beat_goesout, yend = fitted(ipv_model)), lty =2)
```

## Interpreting a regression model

```{r size = "tiny"}
coef(ipv_model)
```

\begin{scriptsize} On average, women in countries where women have higher levels of secondary education have lower levels of acceptance of domestic violence. For example, the model predicts that $\hat{y}=\beta_0 =$ `r round(coef(ipv_model)[1],2)` percent of women in a country in which zero percent of women have a secondary education approve of a husband beating a wife if she goes out without telling him. In a country where 20 percent of women have a secondary education, by contrast, this model predicts that $\hat{y} = \beta_0 + \beta_1 \times 20 =$ `r round(coef(ipv_model)[1] + 20 * coef(ipv_model)[2] ,2)` percent of women approve of intimate partner violence for a women going out without notifying her husband, a clear decline. There is a negative linear relationship between average levels of secondary schooling and women's attitudes about intimate partner violence across countries. \end{scriptsize}


