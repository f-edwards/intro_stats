---
title: "Linear Regression"
author: "Frank Edwards"
date: "11/9/21"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(MASS)
select<-dplyr::select
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "small")
```


## Ordinary least squares

We usually fit a linear regression using a method called *ordinary least squares*, or OLS. \pause

- This method seeks to minimize the distance between $\hat{Y}$ and $Y$. \pause
- To do so, we minimize the sum of squared residuals (SSR) \pause

In other words, we solve for the values of $\beta_0$ and $\beta_1$ that results in the smallest possible value for:

\[ \textrm{SSR} = \sum_{i=1}^n \varepsilon_i^2 =  \sum_{i=1}^n (y_i - \beta_0 - \beta_1 X )^2\] \pause

Also note that we can estimate the coefficient vector $\beta_1$ using matrix algebra:

\[\beta= (X^{\rm T} X )^{-1} X^{\rm T} Y \]

See Imai for a more details on the math behind OLS

## Estimating a regression model in R, the basics

```{r fig.height = 2, size = "tiny"}
x<-c(1, 2, 3, 4, 5)
y<-c(2, 5, 1, 8, 10)

model_demo<-lm(y~x)

coef(model_demo)
```

## The observed data

```{r fig.height = 2, fig.width = 3, size = "tiny"}
ggplot(data.frame(x = x, y = y),
       aes(x = x, y = y)) + 
  geom_point()
```

## The regression line

```{r fig.height = 2, , fig.width = 3, size = "tiny"}
ggplot(data.frame(x = x, y = y),
       aes(x = x, y = y)) + 
  geom_point() + 
  geom_abline(intercept = coef(model_demo)[1], 
              slope = coef(model_demo)[2])
```

## Asking a question with regression

*Are secondary school completion rates for women associated with lower levels of acceptance of intimate partner violence?*  \pause

- What is the outcome variable (y)? What is the predictor variable (x)? \pause

E[acceptance of intimate partner violence] = Intercept + Slope $\times$ secondary school completion \pause

- What is our implied prediction about the slope? 

## Estimating a regression model in R

```{r size = "tiny"}
ipv<-read.csv("https://raw.githubusercontent.com/f-edwards/intro_stats/master/data/dhs_ipv.csv")
## models take the general form
## lm(outcome ~ predictor, data)
ipv_model<-lm(beat_goesout ~ sec_school, 
              data = ipv)

coef(ipv_model)
```

- What does the intercept coefficient ($\beta_0$) indicate? \pause
- What does the slope coefficient ($\beta_1$) indicate?

## Visualize the model

```{r echo = F}
ggplot(ipv %>% 
         filter(!(is.na(sec_school)), !(is.na(beat_goesout))), 
       aes(x=sec_school, y = beat_goesout)) + 
  geom_point() + 
  geom_abline(aes(intercept = coef(ipv_model)[1], 
                  slope = coef(ipv_model)[2])) #+ 
  # geom_point(aes(x = sec_school, y = fitted(ipv_model)), color = "red") + 
  # geom_segment(aes(x = sec_school, xend = sec_school, 
  #                  y = beat_goesout, yend = fitted(ipv_model)), lty =2)
```

## Visualize the model: expected values of y

```{r echo = F}
ggplot(ipv %>% 
         filter(!(is.na(sec_school)), !(is.na(beat_goesout))), 
       aes(x=sec_school, y = beat_goesout)) + 
  geom_point() + 
  geom_abline(aes(intercept = coef(ipv_model)[1], 
                  slope = coef(ipv_model)[2])) + 
  geom_point(aes(x = sec_school, y = fitted(ipv_model)), color = "red") #+ 
  # geom_segment(aes(x = sec_school, xend = sec_school, 
  #                  y = beat_goesout, yend = fitted(ipv_model)), lty =2)
```

## Visualize the model: error term (residuals)

```{r echo = F}
ggplot(ipv %>% 
         filter(!(is.na(sec_school)), !(is.na(beat_goesout))), 
       aes(x=sec_school, y = beat_goesout)) + 
  geom_point() + 
  geom_abline(aes(intercept = coef(ipv_model)[1], 
                  slope = coef(ipv_model)[2])) + 
  geom_point(aes(x = sec_school, y = fitted(ipv_model)), color = "red") + 
  geom_segment(aes(x = sec_school, xend = sec_school,
                   y = beat_goesout, yend = fitted(ipv_model)), lty =2)
```

## Interpreting a regression model

```{r size = "tiny"}
coef(ipv_model)
```

\begin{scriptsize} On average, women in countries where women have higher levels of secondary education have lower levels of acceptance of domestic violence. For example, the model predicts that $\hat{y}=\beta_0 =$ `r round(coef(ipv_model)[1],2)` percent of women in a country in which zero percent of women have a secondary education approve of a husband beating a wife if she goes out without telling him. In a country where 20 percent of women have a secondary education, by contrast, this model predicts that $\hat{y} = \beta_0 + \beta_1 \times 20 =$ `r round(coef(ipv_model)[1] + 20 * coef(ipv_model)[2] ,2)` percent of women approve of intimate partner violence for a women going out without notifying her husband, a clear decline. There is a negative linear relationship between average levels of secondary schooling and women's attitudes about intimate partner violence across countries. \end{scriptsize}


## Linear regression with multiple predictors

We can extend the linear regression model:

\[ y = \beta_0 + \beta_1X + \varepsilon\] \pause

to include more than one predictor. We rewrite the equation as:

\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \cdots \beta_p x_p + \varepsilon\] \pause

## In matrix notation

To be more compact:

\[ Y = \beta X + \varepsilon\]

Where $Y$ is the vector of predictors, $\beta$ is the vector of coefficients (including the intercept), $X$ is the matrix of all predictors, and $\varepsilon$ is the error term.

## Linear regression with multiple predictors

Our first model, for country $i$, was:

\[\textrm{IPV Attitudes}_i = \beta_0 + \beta_1\textrm{Secondary School}_i + \varepsilon\] \pause

Let's add a predictor for region. Remember from prior examples that we saw clear patterns within regions.  

## Estimating a multiple linear regression in R

```{r}
ipv_model2<-lm(beat_goesout ~ sec_school + region, 
               data = ipv)
```

## Interpreting a regression model with multiple coefficients

```{r size = "tiny"}
coef(ipv_model2)
```

Now, we have a coefficient for secondary school, in addition to a coefficient for each region. Note that this kind of model requires a "reference category", which is left out. In this case, Asia is the reference.  

## Interpreting a regression model with multiple coefficients

```{r size = "tiny"}
coef(ipv_model2)
```

Recall that $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$

What do we predict will be the level of tolerance for IPV among women 

- if sec_school = 50 and region = Latin America \pause
- if sec_school = 50 and region = Middle East and Central Asia 


## Visualizing the model

```{r echo = FALSE}
ipv_new<-ipv %>% 
  filter(!(is.na(sec_school)), !(is.na(beat_goesout)))

coefs<-coef(ipv_model2)

ggplot(ipv_new,
       aes(x = sec_school, y = beat_goesout, color = region)) + 
  geom_point() + 
  geom_abline(aes(intercept = coefs[1], slope = coefs[2]), color ="#F8766D") + 
  geom_abline(aes(intercept = coefs[1] + coefs[3], slope = coefs[2]), color ="#7CAE00") + 
  geom_abline(aes(intercept = coefs[1] + coefs[4], slope = coefs[2]), color ="#00BFC4") + 
  geom_abline(aes(intercept = coefs[1] + coefs[5], slope = coefs[2]), color ="#C77CFF") 
```

## Interactions

The prior model allowed each region to have its own starting level of tolerance for IPV. What if we thought the relationship (effect) of secondary schooling on IPV depended on region?

We can add *interaction terms* to our model to model processes where we believe the relationship between $y$ and $x_1$ is a function of $x_2$. 

\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \varepsilon\]

## Estimating interactions in R

```{r size = "tiny"}
ipv_model3<-lm(beat_goesout ~ sec_school + region + 
                 region * sec_school,
               data = ipv)
```

### Interpreting an interaction model

```{r}
coef(ipv_model3)
```

## How interactions work

- What is the predicted level of IPV tolerance in a country where sec_school = 20 in Latin America?
- In Sub-Saharan Africa?

Recall that Asia is the reference category

## Visualizing interactions

```{r echo = FALSE}
ipv_new<-ipv %>% 
  filter(!(is.na(sec_school)), !(is.na(beat_goesout)))

coefs<-coef(ipv_model3)

ggplot(ipv_new,
       aes(x = sec_school, y = beat_goesout, color = region)) + 
  geom_point() + 
  geom_abline(aes(intercept = coefs[1], slope = coefs[2]), color ="#F8766D") + 
  geom_abline(aes(intercept = coefs[1] + coefs[3], slope = coefs[2] + coefs[6]), color ="#7CAE00") + 
  geom_abline(aes(intercept = coefs[1] + coefs[4], slope = coefs[2] + coefs[7]), color ="#00BFC4") + 
  geom_abline(aes(intercept = coefs[1] + coefs[5], slope = coefs[2] + coefs[8]), color ="#C77CFF") 
```

## Let's try this with different data

```{r}
### load in the 'mtcars' data
data(mtcars)
head(mtcars)
```

## Variables in the mtcars data

- mpg	Miles/(US) gallon
-	cyl	Number of cylinders
-	disp	Displacement (cu.in.)
-	hp	Gross horsepower
-	drat	Rear axle ratio
-	wt	Weight (1000 lbs)
-	qsec	1/4 mile time
-	vs	Engine (0 = V-shaped, 1 = straight)
-	am	Transmission (0 = automatic, 1 = manual)
-	gear	Number of forward gears
-	carb	Number of carburetors

## Let's build a model for fuel efficiency with this data (theory!)

Our outcome of interest is `mpg`. What measured features of these cars do we think might be related to fuel efficiency?

## Start simple

$$ \textrm{E}[\textrm{mpg}_i] = \beta_0 + \beta_1 \textrm{hp}_i $$ \pause
$$\textrm{mpg}_i = \beta_0 + \beta_1 \textrm{hp}_i + \varepsilon $$

How would we estimate this model in R? 

## Lab: Improve model fit for mpg

How can we make a better model?

## Lab part 2: transforming predictors

Load the gapminder data

```{r}
library(gapminder)
```

Let's build a model for life expectancy. \pause

$$ \textrm{lifeExp}_i = ?? $$
What is life expectancy a function of?

## Lab part 3: Prediction

- using `predict()`
- Visualizing model inferences

## Conclusion

- Regression models are at the core of social science methodology. Get comfortable with them. 
- All models are wrong, some are useful. Reality is rarely accurately described by straight lines, but we can learn a lot from them.
- Think carefully about your modeling decisions. Connect your models to your theory about how a process works.


