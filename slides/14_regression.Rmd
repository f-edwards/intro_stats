---
title: "Introducing..... Linear Regression!"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(MASS)
select<-dplyr::select
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = F, size = "small")
```

## Lines

We can define a line as:

\[y = mx + b\]

Where $m$ is the slope and $b$ is the y-intercept.


## Lines: slopes

$y = x + 2$ 

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = 2)
```

## Lines: slopes

$y = 2x + 2$

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 2, intercept = 2)
```


## Lines: slopes

$y = -2x + 2$

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = -3, intercept = 2)
```

## Lines: slopes

$y = 0.2x + 2$

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 0.2, intercept = 2)
```

## Lines: intercepts

$y = x + 0$

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = 0)
```

## Lines: intercepts

$y = x + 2$

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = 2)
```

## Lines: intercepts

$y = x - 3$

```{r echo = F, fig.height = 2, fig.width = 2}
ggplot(data.frame(x = -10:10, y = -10:10),
       aes(x = x, y = y)) + 
  geom_point(alpha = 0) + 
  geom_abline(slope = 1, intercept = -3)
```

## The linear regression model

We can describe the relationship between a predictor variable $X$ and an outcome variable $Y$ with a line:

\[y = mx + b\] 

What does m describe? \pause

- The increase in y for a one-unit increase in x

What does b describe \pause

- The location of y when x = 0


## The linear regression model: expected value

We can describe the relationship between a predictor variable $X$ and the expected value $E$ of an outcome variable $Y$ with the line:

\[ E[Y] = \beta_0 + \beta_1 X  \] \pause

What does $\beta_0$ describe? \pause

What does $\beta_1$ describe?

## The error term in linear regression

We can describe the relationship between a predictor variable $X$ and an outcome variable $Y$ with the line:

\[ Y = \beta_0 + \beta_1 X + \varepsilon \]

Where $\beta_0$ is the y-intercept of the line, $\beta_1$ is the slope of the line, and $\varepsilon$ is the error between the fitted line and the coordinates $(X,Y)$

## The linear regression model

\[ Y = \beta_0 + \beta_1 X + \varepsilon \]

$\beta_0$: The value of $y$ when $x$ is equal to zero \pause

$\beta_1$: The average increase in $y$ when $x$ increases by one unit \pause

$\varepsilon$: The distance between the line $y = \beta_0 + \beta_1 X$ and the actual observed values of $y$. \pause

The line $y = \beta_0 + \beta_1 X$ provides an expected value for $y$ based on the values of $x$.

## The linear regresion model as a prediction engine

- If $\beta_0 = 2$ and $\beta_1 = 1.5$, what is the expected value of $y$ when $x = 4$? \pause

- When $x = 2$?

## The linear regression model and prediction

We put a $\hat{hat}$ on variables to indicate that they are estimated from the data, or predicted.

A regression line predicts values $Y$, $\hat{Y}$ with the equation:

$\hat{Y} = \beta_0 + \beta_1 X$

and the residual, or prediction error is the difference between the observed and predicted values of $Y$

$\varepsilon = Y_{obs} - \hat{Y}$

## Understanding the regression line for real data

```{r echo = FALSE, size = "tiny"}
r<-0.95
sample <- mvrnorm(n=10, 
                  mu=c(1, 1), 
                  Sigma=matrix(c(1, r, r, 1), nrow=2), 
                  empirical=TRUE)

sample<-data.frame(x = sample[,1], y = sample[,2])

m1<-lm(y~x, data = sample)
coefs<-coef(m1)

as_tibble(sample)
```

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

- Estimate $\hat{Y}$. Recall that $\hat{Y} = \beta_0 + \beta_1 X$ \pause
- Estimate $\varepsilon$. Recall that $\varepsilon = Y - \hat{Y}$

## Understanding the regression line

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() #+ 
  #geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  #geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding the fit

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) #+ 
  #geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding $\hat{y}$

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2)
```

## Understanding the regression line: adding $\varepsilon$

$\beta_0 =$ `r coefs[1]`, $\beta_1 =$ `r coefs[2]`

```{r echo = FALSE, fig.height = 4}
yhat<-fitted(m1)
ggplot(sample, aes(x=x,y=y)) + 
  geom_point() + 
  geom_abline(slope = coefs[2], intercept = coefs[1]) + 
  geom_point(aes(x=x, y = yhat), color = 2, size = 2)  +
  geom_segment(aes(x = x, xend =x, y = y, yend = yhat), lty =2)
```

## Ordinary least squares

We usually fit a linear regression using a method called *ordinary least squares*, or OLS. \pause

- This method seeks to minimize the distance between $\hat{Y}$ and $Y$. \pause
- To do so, we minimize the sum of squared residuals (SSR) \pause

In other words, we solve for the values of $\beta_0$ and $\beta_1$ that result in the smallest possible value for SSR

\[ \textrm{SSR} = \sum_{i=1}^n \varepsilon_i^2 =  \sum_{i=1}^n (y_i - \beta_0 - \beta_1 X )^2\] \pause

## Estimating a regression model in R, the basics

Let's use `iris` to estimate this regression model:

\[ \textrm{Sepal.Length} = \beta_0 + \beta_1 \textrm{Petal.Length} + \varepsilon \]

```{r}
my_cool_model<-lm(Sepal.Length ~ Petal.Length,
                  data = iris)

my_cool_model
```

## What does this model tell us? 

- $\beta_0 = 4.3$ \pause
- $\beta_1 = 0.4$ \pause

\[E(\textrm{Sepal.Length}) = \beta_0 + \beta_1 \textrm{Petal.Length} \]

## Visualized: observed data

```{r echo = FALSE, fig.height = 4}
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length)) + 
  geom_point() 
```

## Visualized: regression line

```{r echo = FALSE, fig.height = 4}
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length)) + 
  geom_point() + 
  geom_abline(slope = coef(my_cool_model)[2], 
              intercept = coef(my_cool_model)[1])
```

## Visualized: expected values (yhat)

```{r echo = FALSE, fig.height = 4}
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length)) + 
  geom_point() + 
  geom_abline(slope = coef(my_cool_model)[2], 
              intercept = coef(my_cool_model)[1]) + 
  geom_point(aes(x=Petal.Length, 
                 y = fitted(my_cool_model)), color = 2, size = 2) 
```

## Visualized: residuals (epsilon)

```{r echo = FALSE, fig.height = 4}
ggplot(iris, aes(x=Petal.Length,y=Sepal.Length)) + 
  geom_point() + 
  geom_abline(slope = coef(my_cool_model)[2], 
              intercept = coef(my_cool_model)[1]) + 
  geom_point(aes(x=Petal.Length, 
                 y = fitted(my_cool_model)), color = 2, size = 2)  +
  geom_segment(aes(x = Petal.Length, 
                   xend = Petal.Length, 
                   y = Sepal.Length, 
                   yend = fitted(my_cool_model)), lty =2)
```