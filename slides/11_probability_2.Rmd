---
title: "Probability, 2"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, include=FALSE}
rm(list=ls())
library(tidyverse)
library(qss)
library(pander)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "tiny")
```

## Random Variables

- Random variables assign values to events \pause
- Each value is mutually exclusive \pause
- The set of all values is exhaustive (the sample space $\Omega$) \pause
- Discrete random variables take a finite number of values (e.g. TRUE, FALSE) \pause
- Continuous random variables are real numbers, and take on an infinite number of values 

## The simplest random variable: the binary Bernoulli

Any random variable with two values is called a Bernoulli random variable. \pause

Bernoulli (binary) variables are typically represented as $[0,1]$ or $[T,F]$. They can also be two-level character variables, like $[\textsf{pass, fail}]$ or $[\textsf{plaid, stripes}]$.

## A coin flip as a Bernoulli random variable

A coin flip can be defined as a discrete random variable $X$ \pause

- If the coin lands on heads, $X=1$
- If the coin lands on tails, $X=0$ \pause

The probability of a Bernoulli variable is the probability of success, or $X=1$ \pause

$P(X=1) = p$

## Random variable (probability distribution) notation

\[X \sim Bernoulli(p)\] \pause

Reads: X is a Bernoulli distributed random variable with probability p \pause

In this notation, we name the variable $X$, note that it is randomly distributed $\sim$, name the distribution it follows $Bernoulli$, and list the parameters for that distribution $p$.

## Let's flip some coins

```{r}
sample_of_flips<-rbinom(5, 1, 0.5)
pander(table(sample_of_flips))
```
\pause

This is the result of taking 5 draws from a Bernoulli random variable with probability $0.5$. 

## Describing a probability distribution: probability mass

We use a probability mass function to show how likely each value is in a random variable

The probability mass function (PMF) of a variable $X$ is defined as the probability that a variable takes on a particular value $x$. 

\[PMF(x) = P(X=x)\]

For a Bernoulli variable, $PMF(X=1)=p$ and $PMF(X=0)=1-p$

## The probability mass function for our coin flip

\[PMF(X=1)=p=0.5\]

\[PMF(X=0)=1-p=0.5\]

```{r echo=FALSE, fig.height=3}

temp_dat<-data.frame(x=c("0","1"), p=0.5)
ggplot(temp_dat, aes(x=x, y=p)) +
  geom_col()

```

## The probability mass function for passing the bar in NJ (p=0.653)

\[PMF(X=1)=p=0.653\]

\[PMF(X=0)=1-p=0.347\]

```{r echo=FALSE, fig.height=3}
temp_dat<-data.frame(x=c("0","1"), p=c(0.347,0.653))
ggplot(temp_dat, aes(x=x, y=p)) +
  geom_col()
```

## Describing a probability distribution: cumulative probability

How likely is a variable to take a value less than or equal to a specified value? 

We define the cumulative distribution function as the sum of all probabilities up to a value x

\[CDF(X) = P(X \leq x) = \sum_{k \leq x}PMF(k)\]

The CDF always ranges from 0 to 1, and never decreases as $x$ increases. 

```{r echo=FALSE, fig.height=3}
temp_dat<-data.frame(x1=c(-0.5, 0, 1),
                     x2=c(0, 1, 1.5),
                     y1=c(0, 0.653, 1),
                     y2=c(0, 0.653, 1))

ggplot(temp_dat,
       aes(x=x1,y=y1,
           xend=x2, yend=y2)) + 
  geom_segment() +
  ylab("P(X <= x)") +
  xlab("x")
```

## Uniform random variables

Uniform random variables have an equal probability of taking any real value within a given interval $[a,b]$. \pause

What does $X \sim Uniform(0,10)$ look like? \pause

## Uniform random variables

Let's simulate it! 10 draws

```{r fig.height=3}
unif_draws<-runif(10, min=0, max=10)
unif_draws
hist(unif_draws, freq=F)
```

## Uniform random variable: 100 draws

```{r echo=F}
unif_draws<-runif(100, min=0, max=10)
hist(unif_draws, freq=F)
```

## Uniform random variable: 10000 draws

```{r echo=F}
unif_draws<-runif(10000, min=0, max=10)
hist(unif_draws, freq=F)
```

## Properties of uniform random variables

For a uniform random variable on the interval $[a,b]$, the probability of drawing any value between $a$ and $b$ is

\[\frac{1}{b-a}\]

Formally, the PDF (density, not mass for continuous) and CDF are defined as 

\[ \textsf{PDF:}  \begin{cases}
                  \frac{1}{b - a} & \text{for } x \in [a,b]  \\
                  0               & \text{otherwise}
                \end{cases} \]
                
 \[\textsf{CDF:} \begin{cases}
                  0               & \text{for } x < a \\
                  \frac{x-a}{b-a} & \text{for } x \in [a,b) \\
                  1               & \text{for } x \ge b
                \end{cases} \]
                
## Probability Density function for $X \sim Uniform(0,10)$

```{r echo = F}
b<-10; a<-0
p<-1/(b-a)
temp_dat<-data.frame(x1=c(-5, 0, 10),
                     x2=c(0, 10, 15),
                     y1=c(0, p, 0),
                     y2=c(0, p, 0))
ggplot(temp_dat, aes(x=x1, xend=x2, y=y1, yend=y2)) + 
  geom_segment() + 
  xlab("x") + 
  ylab("Density of P(x=X)")
```

## Cumulative Distribution Function for $X \sim Uniform(0,10)$

```{r echo = F}
b<-10; a<-0
p<-1/(b-a)
temp_dat<-data.frame(x1=c(-5, 0, 10),
                     x2=c(0, 10, 15),
                     y1=c(0, 0, 1),
                     y2=c(0, 1, 1))
ggplot(temp_dat, aes(x=x1, xend=x2, y=y1, yend=y2)) + 
  geom_segment() + 
  xlab("x") + 
  ylab("P(x<=X)")
```

```{r echo = F}
b<-10; a<-0
p<-1/(b-a)
temp_dat<-data.frame(x1=c(-5, 0, 10),
                     x2=c(0, 10, 15),
                     y1=c(0, p, 0),
                     y2=c(0, p, 0))
ggplot(temp_dat, aes(x=x1, xend=x2, y=y1, yend=y2)) + 
  geom_segment() + 
  xlab("x") + 
  ylab("p")
```

## A note on CDF for continuous variables                

Recall that a CDF for a discrete variable is the sum of all probabilities for values $x\leq X$

We can't sum over each value when $X$ is continuous. Instead, we'll take the integral

\[CDF(x) = P(x\leq X) = \int_{-\infty}^x PDF(x) dx \]

## The binomial distribution

When we repeat Bernoulli trials many times, we get a binomial random variable. \pause

Binomial random variables represent the count of successes in a fixed number of trials of a Bernoulli experiment.

Formally:

*A binomial random variable is the sum of n independently and identically distributed (i.i.d) Bernoulli random variables.* \pause

Binomial variables take on integer values between $0$ and $n$

## Back to flipping coins

Imagine we flipped a coin 5 times, and then repeated the exercise twice more

```{r echo = F}
## trial 1
rbinom(5,1,0.5)
## trial 2
rbinom(5,1,0.5)
## trial 3
rbinom(5,1,0.5)

```

Each of these trials is a sample from  $X \sim Binomial(n, p)$ where $n=5$ and $p=0.5$ \pause

What is $x$ for each trial?

## Probability Distribution Function for $X \sim Binomial(5, 0.5)$

```{r echo = F}
temp_dat<-data.frame(x=0:5)
temp_dat$p<-dbinom(temp_dat$x, 5, 0.5)
ggplot(temp_dat, 
       aes(x=x, y=p)) + 
  geom_col() +
  ylab("P(x=X)")
```

## Cumulative Distribution Function for $X \sim Binomial(5, 0.5)$

```{r echo = F}
temp_dat$pdf<-pbinom(temp_dat$x, 5, 0.5)

ggplot(temp_dat,
       aes(x = x, xend=x+1, y=pdf, yend=pdf)) + 
  geom_segment() + 
  ylab("P(x<=X)")
```

## The Normal Distribution

The Normal (Gaussian) distribution is continuous, and takes on values from $[-\infty, \infty]$. It has two parameters, the mean $\mu$ and standard deviation $\sigma$ (or variance $\sigma^2$).

- $\mu$ determines the location of the distribution
- $\sigma$ determines the spread of the distribution

## The Normal PDF

```{r echo = F}
sequence<-seq(-8,8,0.001)
n1<-data.frame(x=sequence, y=dnorm(sequence, 0, 1), 
               name = "Mu=0, sd=1")
n2<-data.frame(x=sequence, y=dnorm(sequence, 2, 1), 
               name = "Mu=2, sd=1")
n3<-data.frame(x=sequence, y=dnorm(sequence, 0, 2), 
               name = "Mu=0, sd=2")


plot_dat<-bind_rows(n1, n2, n3)

ggplot(plot_dat,
       aes(x = x, y = y, color = name)) + 
  geom_line() + 
  ylab("Density of P(X=x)")
```

## The Normal CDF

```{r echo = F}
sequence<-seq(-8,8,0.001)
n1<-data.frame(x=sequence, y=pnorm(sequence, 0, 1), 
               name = "Mu=0, sd=1")
n2<-data.frame(x=sequence, y=pnorm(sequence, 2, 1), 
               name = "Mu=2, sd=1")
n3<-data.frame(x=sequence, y=pnorm(sequence, 0, 2), 
               name = "Mu=0, sd=2")


plot_dat<-bind_rows(n1, n2, n3)

ggplot(plot_dat,
       aes(x = x, y = y, color = name)) + 
  geom_line() + 
  ylab("Density of P(X=x)")
```

## Special features of Normal distributions: 

- The sum of many random variables from other distributions are often Normal
- For $X \sim N(\mu, \sigma^2)$, $Z=X+c$ is also Normal: $Z \sim (\mu +c, \sigma^2)$
- $Z=cX$ is distributed $Z\sim N(c\mu, (c\sigma)^2)$
- Z-scores of a Normal random variable are $N(0,1)$

## Area under the curve: interpreting the PDF and CDF

```{r echo = F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dnorm) 
```

## Area under the curve: interpreting the PDF and CDF

```{r echo = F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dnorm) +
  stat_function(fun=dnorm,
                xlim=c(-4,0),
                geom="area",
                fill="blue",
                alpha = 0.5)+
  ggtitle(pnorm(0,0,1))
```

## Area under the curve: interpreting the PDF and CDF

```{r echo = F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=pnorm) +
  geom_point(aes(x=0, y=0.5), color = "blue", size=2)
```

## Area under the curve: interpreting the PDF and CDF

```{r echo = F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dnorm) +
  stat_function(fun=dnorm,
                xlim=c(-4,1),
                geom="area",
                fill="blue",
                alpha = 0.5) +
  ggtitle(round(pnorm(-1,0,1),3))
```

## Area under the curve: interpreting the PDF and CDF

```{r echo = F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=pnorm) +
  geom_point(aes(x=-1, y=0.159), color = "blue", size=2)
```

## Z-scores and area under the curve

Recall that to obtain a z-score, we subtract the mean and divide by the standard deviation:

\[\textsf{z-score} = \frac{X-\mu}{\sigma}\]

For a Normal variable, z-scores are distributed $z \sim N(0,1)$ \pause

What does a z-score of 0 indicate? \pause -1? \pause 2?

## 

```{r echo=F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dnorm) +
  stat_function(fun=dnorm,
                xlim=c(-1,1),
                geom="area",
                fill="blue",
                alpha = 0.5) +
  ggtitle(paste("Mean +/- 1 SD = ",round(pnorm(1,0,1) - pnorm(-1,0,1),3)))
```

##

```{r echo=F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dnorm) +
  stat_function(fun=dnorm,
                xlim=c(-2,2),
                geom="area",
                fill="blue",
                alpha = 0.5) +
  ggtitle(paste("Mean +/- 2 SD = ",round(pnorm(2,0,1) - pnorm(-2,0,1),3)))
```

## 

```{r echo=F}
ggplot(data.frame(x=c(-4,4)), aes(x)) +
  stat_function(fun=dnorm) +
  stat_function(fun=dnorm,
                xlim=c(-3,3),
                geom="area",
                fill="blue",
                alpha = 0.5) +
  ggtitle(paste("Mean +/- 3 SD = ",round(pnorm(3,0,1) - pnorm(-3,0,1),3)))
```

## Useful probability distribution functions

```{r}
### Normal(0,1) probability density function
dnorm(x = 0, mean = 0, sd = 1)
### Normal(0,1) cumulative distribution function
pnorm(q = 0, mean = 0, sd = 1)
### Random draw from a normal(0,1) distribution
rnorm(n = 1, mean = 0, sd = 1)
### CDF position for a given probability (quantile)
qnorm(p = 0.75, mean = 0, sd = 1)
### You can also use dbinom(), pbinom(), rbinom(), qbinom()
```

## The expectation of a random variable

The expectation of a random variable $E(X)$ is the mean of a random variable. 

Be careful not to confuse $E(X)$ and $\bar{x}$. \pause

For a discrete variable, the expectation is the sum of all values of $x$ weighted by their probability, given by the PDF $f(x)$.

\[E(X) = \sum_x x \times f(x)\]

Because continuous variables take on an infinite number of values, we compute the expectation with an integral

\[\int x \times f(x) dx\]

## Variance and standard deviation of a random variable

Recall that for a sample, the standard deviation $sd$

\[sd = \sqrt{\frac{1}{n}\sum_{i=1}^n{(x_i-\bar{x}})^2}\]

And the sample variance is $sd^2$ \pause

For a random variable $X$, the variance is defined via the expectation instead of sample mean

\[V(X) = E[\{X-E(X)\}^2]\] \pause

Note the similarities in the two equations

# Large sample (asymptotic) theorems

## The law of large numbers

As a sample of draws from a random variable increases, the sample mean converges to the population mean $E(X)$

\[\bar{x}_n \rightarrow E(X) \]

## Monte Carlo simulation for the mean of a binomial variable

To test the law of large numbers, let's draw from a binomial variable with varying sample sizes. 

We expect that $\bar{x}$ will converge to $E(X)$ as the sample size $n$ increases

```{r}
## MC simulation, 1000 reps 
sims<-1000
## Take 1000 draws from binomial(0.3, 10)
x<-rbinom(sims, p = 0.3, size = 10)
### output df
out<-data.frame(n=1:sims, xbar = NA)
for(i in 1:sims){
  out$xbar[i]<-sum(x[1:i])/i
}
## or use xbar<-cumsum(x)/1:sims
```

##

```{r echo = F}
ggplot(out, aes(y=xbar, x=n)) +
  geom_line() +
  geom_abline(slope=0, intercept = 3, lty=2)
```

## The Central Limit Theorem

- As $n$ increases, the distribution of the sample mean $\bar{x}$ approaches a Normal distribution. \pause
- This relationship holds for many distributions (Bernoulli, Binomial, Normal, others we'll discuss later) \pause
- If our samples are independent, and each observation within the sample is iid, the distribution of z-scores of sample means converges to a $Normal(0,1)$ distribution \pause
- The Central Limit Theorem allows us to make statements about uncertainty when we haven't observed the population mean or variance

## Monte Carlo simulations of a binomial variable p=0.7, n=10

```{r size = "tiny"}
### Binomial random variable, 10 observations, probability of success = 0.7
### E(X) = np = 0.7 * 10
### pretend this is classes with size 10, probability of passing = 0.7
## sample size = 10 classes of 10 students, size = 100 classes, size = 1000 classes
## simulate each sample size for 1000 replications
sims<-1000
n<-10
p<-0.7
xbar_10<-rep(NA, 10)
xbar_100<-rep(NA, 100)
xbar_1000<-rep(NA, 1000)
for(i in 1:10){
  x_10<-rbinom(10, p=p, size=n)
  xbar_10[i]<-(mean(x_10))
}

for(i in 1:100){
  x_100<-rbinom(100, p=p, size=n)
  xbar_100[i]<-(mean(x_100))
}

for(i in 1:1000){
  x_1000<-rbinom(1000, p=p, size=n)
  xbar_1000[i]<-(mean(x_1000))
}
```

## 

```{r echo = FALSE}
ggplot(data.frame(z_score=scale(xbar_10)), 
       aes(x=z_score))+
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun=dnorm) +
  ggtitle("10 samples") +
  xlim(c(-3,3))
```

## 

```{r echo = FALSE}
ggplot(data.frame(z_score=scale(xbar_100)), 
       aes(x=z_score))+
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun=dnorm) +
  ggtitle("100 samples")+
  xlim(c(-3,3))
```

## 

```{r echo = FALSE}
ggplot(data.frame(z_score=scale(xbar_1000)), 
       aes(x=z_score))+
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun=dnorm) +
  ggtitle("1000 samples")+
  xlim(c(-3,3))
```

## Lab, HW

- Homework (late work): Any late assignments (including this week's HW) are due without penalty by Friday. After that, I'm going to start deducting points for late work.
- Homework: Question 6.6.3. Due 12/4. Start early on this to give yourself time to ask questions
- Lab: Obama vote share example, and some hints for the homework