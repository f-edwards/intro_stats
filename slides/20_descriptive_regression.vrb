\frametitle{When are we just overfitting?}
\phantomsection\label{when-are-we-just-overfitting}
The Bayesian Information Criterion (BIC) provides a check against
overfitting. It evaluates goodness of fit with a penalty for complexity
(count of model parameters), based on the log-likelihood of the model.
The first term \(k\ln(n)\) adjusts for model complexity with \(n\) as
the number of observations and \(k\) as the number of model parameters
(\(\beta\))

\[BIC = k \ln(n)  - 2\ln(\hat{L})\]

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(m0, m1, m2, m3, m4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    df      BIC
## m0 10 23219.69
## m1 12 23211.92
## m2 18 23235.98
## m3 27 23283.77
## m4 72 23545.61
\end{verbatim}

\normalsize
